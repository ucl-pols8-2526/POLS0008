[["index.html", "POLS0008: Introduction to Quantitative Research Methods 2025/26 Welcome Description Structure Timetable and key locations Contact details", " POLS0008: Introduction to Quantitative Research Methods 2025/26 Welcome Welcome to POLS0008: Introduction to Quantitative Research Methods. This is one of the first year core module for students enrolled on one of the degree programmes via the cross-faculty initiative [UCL Social Data Institute (SODA)]. This is open to students on the following degree programmes: BSc Philosophy, Politics and Economics with Social Data Science BA Geography with Social Data Science BSc Population Health Science (Data Science) BSc Social Sciences with Data Science Description This module will introduce students to the key tenets of quantitative methods in the social sciences. It assumes no knowledge of quantitative methods or statistical software. Hence, it caters for students from diverse disciplinary backgrounds and adopts a practical hands-on approach to learning, with tutor supported computer tutorials. The module covers descriptive statistics (central tendency and variation), data visualisation, data access, probability, sampling, hypothesis testing, inferential statistics and ends with an introduction to simple linear regression. Students will be introduced to the R statistical software and work with real-world data. Structure For the first four weeks, we will do a deep-dive covering all facets on Descriptive Statistics and R-programming - this will be taught by Dr Andreas Mastrosavvas. From Week 5 to 10, it will be focused on Inferential Statistics - this will be covered by Professor Stephen Jivraj. Nearing the end of the course, you will be given an assignment i.e., a 3,000 word essay (counting 100% towards your final mark for this course) based on secondary analysis of survey data. This information will be introduced to you accordingly by both the module tutors to help you to mentally prepare for this moment. All lecture notes, recommended reading and seminar learning materials as well as some supplementary video content will be hosted on this webpage. You can download the lecture notes and data sets for the practical and seminar lesson from the table below. Week Downloads Topics 1 [Lecture Notes] [Dataset] Understanding Data 2 [Lecture Notes] [Dataset] Examining Data I 3 [Lecture Notes] [Dataset] Examining Data II 4 [Lecture Notes] Sourcing Data Important note: The solutions will be made available each week at the end of Thursday via email after the seminars are completed. Timetable and key locations The lectures are held every week in-person on Tuesday from 12:00pm to 02:00pm at the Roberts Building G06 Sir Ambrose Fleming LT [MAP] A series of 9 computer seminars will take place every week on Thursday starting from 09:00am, up to 02:00pm. All students have been allocated to one of 10 seminar groups (i.e., group 1, 2, 3, 4, 5, 6, 7, 8, 9, or 10). To avoid any confusion, please go to your group’s designated. You will be working together on the tasks and so please bring your laptops. The table below highlights precise location for the seminars: Seminar Location &amp; Times 1 IOE Bedford Way (20) Room W3.08, 09:00am-10:00am 2 IOE Bedford Way (20) Room W3.06, 10:00am-11:00am 3 Bernard Street (40) Room 410, 10:00am-11:00pm 4 IOE Bedford Way (20) Room W3.06, 11:00am-12:00pm 5 Bernard Street (40) Room 410, 11:00am-12:00pm 6 Bernard Street (40) Room 410, 12:00pm-01:00pm 7 Cruciform Building Foyer 1.02 Seminar Room 2, 12:00pm-01:00pm 8 Bernard Street (40) Room 206, 01:00pm-02:00pm 9 Medical Sciences G40, 01:00pm-02:00pm 10 Bernard Street (40) Room 205, 09:00am-10:0am Postgraduate Seminar Facilitators: Angel Torres Guevara, Giovanni Hollenweger &amp; James Rice Important note: Please bring your own laptops with you to the seminars on Thursday Contact details Dr Andreas Mastrosavvas UCL Research Department of Epidemiology and Public Health Room 345, 1-19 Torrington Place, WC1E 7HB Email: a.mastrosavvas@ucl.ac.uk "],["recommended-reading-list.html", "Recommended Reading List Week 1: Understanding Data Week 2: Examining Data I Week 3: Examining Data II Week 4: Sourcing Data Extra notes", " Recommended Reading List Week 1: Understanding Data Book: [Theory, Basics] Cetinkaya-Rundel, M., and Hardin, J., 2021, Introduction to Modern Statistics, Section I: Introduction to data, Chapter 1: Hello data, Page(s): 12-22. Source: openintro.org/book/ims. Book: [R-programming, Basics] Dalgaard, P., 2008, Introductory Statistics with R, Chapter 1: Basics, Page(s): 1-28. Book: [R-programming, Data Management, Basics] Zuur, A.F., Ieno, E.N., &amp; Meesters, E.H.W.G., 2008, A Beginner’s Guide to R, Chapter 2: Getting Data into R, Page(s): 29-54. Book: [R-programming, Data Management, Basics] Zuur, A.F., Ieno, E.N., &amp; Meesters, E.H.W.G., 2009, A Beginner’s Guide to R, Chapter 3: Accessing Variables and Managing Subsets of Data, Page(s): 57-74. Week 2: Examining Data I Book: [Theory, Basics] Cetinkaya-Rundel, M., and Hardin, J., 2021, Introduction to Modern Statistics, Section II: Exploratory data analysis, Chapter 5: Exploring numerical data, Page(s): 76-95. Source: openintro.org/book/ims. Book: [R-programming, Basics] Dalgaard, P., 2008, Introductory Statistics with R, Chapter 2: Descriptive statistics and graphics, Page(s): 67-93. Book: [R-programming, Data Management, Basics] Zuur, A.F., Ieno, E.N., &amp; Meesters, E.H.W.G., 2008, A Beginner’s Guide to R, Chapter 2: Getting Data into R, Page(s): 29-54. Book: [R-programming, Data Management, Basics] Zuur, A.F., Ieno, E.N., &amp; Meesters, E.H.W.G., 2009, A Beginner’s Guide to R, Chapter 3: Accessing Variables and Managing Subsets of Data, Page(s): 57-74. Book: [R-programming, Generating Graphs, Basics] Zuur, A.F., Ieno, E.N., &amp; Meesters, E.H.W.G., 2009, A Beginner’s Guide to R, Chapter 7: Graphing Tools, Page(s): 127-167. Week 3: Examining Data II Book: [R-programming (Base-R code), Generating Graphs, Basics] Zuur, A.F., Ieno, E.N., &amp; Meesters, E.H.W.G., 2009, A Beginner’s Guide to R, Chapter 7: Graphing Tools, Page(s): 127-167. Book: [R-programming (Base-R code), Basics, Visualisation] Dalgaard, P., 2008, Introductory Statistics with R, Chapter 2: Descriptive statistics and graphics, Page(s): 67-93. Book: [R-programming (Tidyverse code), Basics, Visualisation] Wickham, H., and Grolemund, G., 2017, R for Data Science, Chapter 1: Data visualisation with ggplot2, Page(s): 3-33. Book: [Theory, Basics, Graphics] Cetinkaya-Rundel, M., and Hardin, J., 2021, Introduction to Modern Statistics, Section II: Exploratory data analysis, Chapter 4: Exploring categorical data, Page(s): 61-74. Source: openintro.org/book/ims. Book: [Theory, Basics, Graphics] Cetinkaya-Rundel, M., and Hardin, J., 2021, Introduction to Modern Statistics, Section II: Exploratory data analysis, Chapter 5: Exploring numerical data, Page(s): 76-95. Source: openintro.org/book/ims. Week 4: Sourcing Data Article: [R-programming (Tidyverse code), Data Management] Wickham, 2014, Tidy data, Journal of Statistical Software 59(10). [Link] Book: [R-programming (Tidyverse code), Basics, Data Management] Wickham, H., and Grolemund, G., 2017, R for Data Science, Chapter 3: Data Transformation with dplyr, Page(s): 43-73. Book: [R-programming (Tidyverse code), Basics, Data Management] Wickham, H., and Grolemund, G., 2017, R for Data Science, Chapter 9: Tidy Data with tidyr, Page(s): 147-168. Extra notes For other reading materials beyond what’s mentioned here please read through the Module Outline document which is accessible on Moodle. To follow the examples with the dataset provided in the book “Introductory Statistics with R”. You will need to install a package called “ISwR”. Here is the code chuck for performing such installation. # install the ISwR using install.packages() function install.packages(&quot;ISwR&quot;) # active it using library() function library(&quot;ISwR&quot;) # this should allow to use the datasets ‘thuesen’ and ‘eba1977’ to follow the examples in book # to add data - use the data() function data(&quot;thuesen&quot;) data(&quot;eba1977&quot;) # see the datasets thuesen eba1977 "],["understanding-data.html", "Week 1 Understanding Data 1.1 Introduction 1.2 Instructions for accessing RStudio Server 1.3 The environment in RStudio 1.4 Using the R-Console as a calculator 1.5 Creating basic objects and assigning values to them 1.6 Data entry in RStudio 1.7 Uploading data to RStudio Server and importing data into the environment 1.8 Working with data in RStudio 1.9 Seminar tasks and questions", " Week 1 Understanding Data 1.1 Introduction Welcome to week 1’s practicals for Introduction to Quantitative Research Methods. This week we will focus on understanding data. The goal for this week’s session is to get you started with using RStudio and getting you to become familiar with its environment. Today’s session aims to introduce you to the basic programming etiquette as well as building your confidence in using RStudio. At the end of this session, you should be able to perform the following: Accessing RStudio Server from a UCL Workstation or remotely Use the R-Console as a basic calculator Loading in data from a CSV &amp; sub-setting it into smaller chunks Handle discrete and categorical data Basic graphical visualisation in RStudio Make sure to download the data set for Week 1 from HERE if you have not already done so from the Welcome Page, as we will use them later on in Sections 1.7. and 1.9.. The data sets are: For the tutorial practicals (Section 1.7.): Primary Schools in Ealing.csv For the seminar tasks and questions (Section 1.9.): All Schools in London.csv At the end of this tutorial, you will be asked to complete the tasks and questions in Section 1.9. Do try to complete this before Thursday’s seminar session. The solutions to Section 1.9 will be released with the week’s new lecture notes and other content. 1.2 Instructions for accessing RStudio Server To begin, we provide a step-by-step guide to accessing RStudio Server on your personal computer. Please note that you must be connected to UCL’s Network. OPTION A - Connecting to RStudio Server from laptop/PC when connected directly to UCL’s EDUROAM: Make sure you are already connected to UCL’s EDUROAM wifi if you are on campus. Open any web browser (i.e., Internet Explorer, Google Chrome, Safari etc.) and navigate to the browser and type the following link: https://rstudio.data-science.rc.ucl.ac.uk/. Log in with your usual UCL username and password. You should see the RStudio interface appear. OPTION B - Connecting from a personal device: Alternatively, if you are working on your laptop/PC off-campus, you can still access RStudio Server. However, you will need to work remotely via Desktop @ UCL Anywhere from your personal device and connect through there. Video explanation (Length: 12:04 minutes) [Watch on YouTube] Here are the following steps: Initiate UCL Remote Desktop @ Anywhere, click on this LINK Next, click on the ‘Log in to Desktop @ UCL Anywhere’ blue button to log in, and then select “Use Light Version” or “Web Browser”. A page will appear asking you to enter your UCL username and password, enter these pieces of information, and click “Log On”. Click on the icon “Desktop @ UCL Anywhere” to initiate the remote access on to a UCL workstation You are now in the network remotely. To open RStudio Server simply open any web browser (i.e., Internet Explorer or Google Chrome) within the remote environment and go to this link: https://rstudio.data-science.rc.ucl.ac.uk/. Log in with your usual UCL username and password and you should see the RStudio interface appear. 1.3 The environment in RStudio When opening RStudio for the first time, you are greeted with its interface. The window is split into three panels: 1.) R-Console, 2.) Environments and 3.) Files, help &amp; Output. Panel 1: The R-Console lets the user type in R-codes to execute rapid commands and use it as basic calculator. Panel 2: The Environments lets the user see which data sets, objects and other files are currently stored in RStudio’s memory Panel 3: Under the File tab, it lets the user access other folders stored in the computer to open datasets. Under the Help tab, it also allows the user to view the help menu for codes and commands. Finally, under the Plots tab, the user can perusal his/her generated plots (e.g., histogram, scatter plots, maps etc.). The above section is the Menu Bar. You can access other functions for saving, editing, and opening a new Script File for writing codes. When you open a Script File, it reveals a fourth panel above the R-Console. You can open a Script File by simply going to the Menu Bar and clicking on “New File” &gt;&gt; “R Script”. This should open a new Script File titled ‘Untitled 1’. In all practical tutorials, you will be encouraged to use an R Script for collating and saving the codes written for these analyses. We will start writing codes in a script from section 1.7 on wards. For now, let us start with the absolute basics, we begin with interacting with the R-Console as a basic calculator and typing in some simple code. 1.4 Using the R-Console as a calculator The R console window (i.e., Panel 1) is the place where RStudio is waiting for you to tell it what to do. It will show the code you have commanded RStudio to execute, and it will also show the results from that command. You can type the commands directly into the window for execution of as well. Video explanation (Length: 13:35 minutes) [Watch on YouTube] Let us start by using the console window as a basic calculator for typing in addition (+), subtraction (-), multiplication (*), division (/) and performing other complex sums. Click inside the R Console window and type 19+8, and press enter button to get your answer. Perform the following sums by typing them inside the R Console window: # Addition 19+8 ## [1] 27 # Subtraction 20-89 ## [1] -69 # Multiplication 18*20 ## [1] 360 # Division 27/3 ## [1] 9 # Complex sums (5*(170-3.405)/91)+1002 ## [1] 1011.154 Important Note: The text that follows after the hash tag # in the above code chunk is a comment and NOT actual code. I have put those comments there to tell you what the code below it (i.e., without hash tag # in front of it) what its doing. Aside from basic arithmetic operations, we can use some basic mathematical functions such as the exponential and natural logarithms: exp() is the exponential function log() is the logarithmic function ^ this symbol allows the user to raise a number to a power Do not worry at all about these function as you will use them later in the weeks to come for transforming variables. For now, perform the following by typing them inside the R-Console window: # Exponential exp(5) ## [1] 148.4132 # Natural logarithm log(3) ## [1] 1.098612 # Raising a number to a power e.g., 2 raise to the power of 8 2^8 ## [1] 256 1.5 Creating basic objects and assigning values to them Now that we are a bit familiar with using the console as a calculator. Let us build on this and learn one of the most important codes in RStudio which is Assignment Operator. Video explanation (Length: 20:29 minutes) [Watch on YouTube] This arrow symbol &lt;- is called the Assignment Operator. It is typed by pressing the less than symbol &lt; followed by the hyphen symbol -. It allows the user to assign values to an object. Objects are defined as stored quantities in RStudio’s environment. These objects can be assigned anything from a numeric value to a string character. For instance, suppose we want to create a numeric object called x and assign it with a value of 3. We do this by typing x &lt;- 3. Another example, suppose we want to create a string object called y and we assign it with some text \"Hello!\". We do this typing y &lt;- \"Hello!\". Let us create the objects a, b, c, and d and assign them with numeric values. Perform the following by typing them inside the R Console window: # Create an object called &#39;a&#39; and assign the value 17 to it a &lt;- 17 # Type the object &#39;a&#39; in console as a command to return value 17 a ## [1] 17 # Create an object called &#39;b&#39; and assign the value 10 to it b &lt;- 10 # Type the object &#39;b&#39; in console as a command to return value 10 b ## [1] 10 # Create an object called &#39;c&#39; and assign the value 9 to it c &lt;- 9 # Type the object &#39;c&#39; in console as a command to return value 9 c ## [1] 9 # Create an object called &#39;d&#39; and assign the value 8 to it d &lt;- 8 # Type the object &#39;d&#39; in console as a command to return value 8 d ## [1] 8 Notice how the objects a, b, c and d and its value are stored in RStudio’s environment panel. We can perform the following arithmetic operations with these object values: # Use the objects a, b, c &amp; d to type the following maths (a + b + c + d)/5 ## [1] 8.8 # Use the objects a, b, c &amp; d to type the following maths (5*(a-c)/d)^2 ## [1] 25 Let us create more objects but this time we will assign character string(s) to them. Please note that when typing a string of characters as data in RStudio, you will need to cover them with quotation marks “…”. For example, suppose we want to create a string object called y and assign it with some text \"Hello!\". We do this by typing y &lt;- \"Hello!\". Try these examples of assigning the following character text to an object. # Create an object called &#39;e&#39; and assign the character string &quot;RStudio&quot; e &lt;- &quot;RStudio&quot; # Type the object &#39;e&#39; in the console as a command to return &quot;RStudio&quot; e ## [1] &quot;RStudio&quot; # Create an object called &#39;f&#39;, assign character string &quot;Hello world&quot; f &lt;- &quot;Hello world&quot; # Type the object &#39;f&#39; in the console as a command to return &quot;Hello world&quot; f ## [1] &quot;Hello world&quot; # Create an object called &#39;g&#39; and assign &quot;Blade Runner is amazing&quot; g &lt;- &quot;Blade Runner is amazing&quot; # Type the object &#39;g&#39; in the console to return the result g ## [1] &quot;Blade Runner is amazing&quot; We are now familiar with using the console and assigning (numeric and string) values to objects. The parts covered in from section 1.3 and 1.4 are the initial building blocks for coding &amp; creating data sets. Let us progress to section 1.5. From this point onwards, we will learn the basics of managing data and etiquettes, which includes creating data frames, as well as importing CSVs &amp; saving R-objects as a CSV file, and setting up a work directory in RStudio server. 1.6 Data entry in RStudio As you have already seen, RStudio is an object-oriented software package and so entering data is slightly different for the usual way when inputting information into a spreadsheet (e.g., Microsoft Excel etc.,). Here, you will need to enter the information as Vector objects before combining them into a Data Frame object. Consider this incredibly Wishie Washy example of some data containing additional health information of 10 people. It contains the variable (or column) names 'id', 'name', 'height', 'weight' and 'gender'. id name height weight gender 1 Sam 1.65 64.2 M 2 Kofi 1.77 80.3 M 3 Kate 1.70 58.7 F 4 Cindy 1.68 75.0 F 5 Patel 1.80 69.6 M 6 James 1.60 49.3 M 7 Tatiana 1.66 52.7 F 8 Roberto 1.71 40.0 M 9 Rubio 1.63 55.6 M 10 Fatima 1.73 62.5 F In RStudio, data is entered as a sequence of elements and listed inside an object called a Vector. For instance, if we have three age values of 12, 57 and 26 years, and we want to enter this in RStudio, we need to use the function c() and combine these three elements into a Vector object. Hence, the code will be c(12, 57, 26). We can assign this to something by typing this code age &lt;- c(12, 57, 26). Any time you type age into RStudio console. It will return these three values unless you chose to overwrite it with different information. Let us look at this more closely with the id variable from the above data. Each person has an ID number from 1 to 10. We are going to list the numbers 1, 2, 3, 4, 5, 6, 7, 8, 9 and 10 as a sequence of elements into a vector using c() and then assign it to as a vector object calling it id. # Create &#39;id&#39; vector object id &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) # Type the vector object &#39;id&#39; in console to return output id ## [1] 1 2 3 4 5 6 7 8 9 10 Now, let us enter the same remaining columns for 'name', 'height', 'weight' and 'gender' like we did for ‘id’: # Create &#39;name&#39; vector object name &lt;- c(&quot;Sam&quot;, &quot;Kofi&quot;, &quot;Kate&quot;, &quot;Cindy&quot;, &quot;Patel&quot;, &quot;Harry&quot;, &quot;Kimba&quot;, &quot;Roberto&quot;, &quot;Rubio&quot;, &quot;Fatima&quot;) # Create &#39;height&#39; (in meters) vector object height &lt;- c(1.65, 1.77, 1.70, 1.68, 1.80, 1.60, 1.66, 1.71, 1.63, 1.73) # Create &#39;weight&#39; (in kg) vector object weight &lt;- c(64.2, 80.3, 58.7, 75.0, 69.6, 49.3, 52.7, 40.0, 55.6, 62.5) # Create &#39;gender&#39; vector object gender &lt;- c(&quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;) Now, that we have the vector objects ready. We can bring them together to create a proper data set. This new object is called a Data Frame. We need to list the vectors inside the data.frame() function. # Create a dataset (data frame) dataset &lt;- data.frame(id, name, height, weight, gender) # Type the data frame object &#39;dataset&#39; in console to see output dataset ## id name height weight gender ## 1 1 Sam 1.65 64.2 M ## 2 2 Kofi 1.77 80.3 M ## 3 3 Kate 1.70 58.7 F ## 4 4 Cindy 1.68 75.0 F ## 5 5 Patel 1.80 69.6 M ## 6 6 Harry 1.60 49.3 M ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 4 rows ] Important Note: The column ‘id’ is numeric variable with integers. The second column ‘name’ is a text variable with strings. The third &amp; fourth columns ‘height’ and ‘weight’ are examples of a numeric variables with real numbers – both variables are continuous. The variable ‘gender’ is text variable with strings – however, this type of variable is classed as a categorical variable as individuals were categorised as either ‘M’ and ‘F’. 1.7 Uploading data to RStudio Server and importing data into the environment We are going to import the downloaded data for Week 1 into RStudio. It contains information pertained to 58 primary schools in Ealing. We can open spreadsheets, particularly, CSV files in an organised way by uploading the file in the RStudio Server and then opening it using the read.csv() function. Video explanation (Length: 12:16 minutes) [Watch on YouTube] We can do this in four steps: Step 1. Keep the space in your personal directory /nfs/cfs/home2/XXXX/ucl_username tidy by creating new folder and naming it Week 1 simply by clicking the New Folder button in the bottom-right panel. Step 2. Next, click on the Week 1 folder to enter it, and then click on the Upload button and select the downloaded CSV file Primary Schools in Ealing.csv to be uploaded into the RStudio Server. Step 3. We must set the directory to the folder location in the server to where the CSV file was uploaded by clicking on the More &gt;&gt; Set As Working Directory. Step 4. Finally, we can import the data using the read.csv() function. The code syntax in the script should read as follows: # Load data into RStudio. The spreadsheet is stored in the object called &#39;SchoolData&#39; SchoolData &lt;- read.csv(&quot;Primary Schools in Ealing.csv&quot;) The loaded dataset contains the following variables: SchoolName: Name of school in Ealing (string) Type: School classified as a ‘Primary’ school (string) AgeGroup: Categorised as three age groups i.e., 3-11, 4-11, 7-11 (categorical) NumberBoys: Total number of boys in a primary school (Discrete/counts) NumberGirls: Total number of girls in a primary school (Discrete/counts) TotalStudents: Total number of students in a primary school (Discrete/counts) OfstedGrade: Overall school performance 1 = excellent, 2 = good, 3 = requires improvement, 4 = inadequate (categorical) We have learned a lot of basic things in RStudio, the stuff shown in this section in particular will be used quite a lot in future tutorials - so do get use to keep your space clean with new folders, and use uploading and importing data. Let us progress to the final section and learn some basics of working with imported data in R. 1.8 Working with data in RStudio 1.8.1 Useful functions One can examine the structure of the imported data with the following basic functions. str(): tells the user which columns in the data frame are character or numeric variables head(): allows the user to see the first top 10 rows of the data frame tail(): allows the user to see the last bottom 10 rows of the data frame ncol(): tells the user the total number of columns present in the data frame nrow(): tells the user the total number of observations (or rows) present in the data frame names(): returns the list column names present in the data frame # Play with code str(SchoolData) ## &#39;data.frame&#39;: 58 obs. of 6 variables: ## $ SchoolName : chr &quot;Berrymede Junior School&quot; &quot;East Acton Primary School&quot; &quot;Oldfield Primary School&quot; &quot;North Ealing Primary School&quot; ... ## $ Type : chr &quot;Primary&quot; &quot;Primary&quot; &quot;Primary&quot; &quot;Primary&quot; ... ## $ NumberBoys : int 180 160 225 340 265 230 155 285 185 150 ... ## $ NumberGirls : int 200 165 240 355 205 205 180 290 190 125 ... ## $ TotalStudents: int 377 329 465 697 471 435 335 573 373 273 ... ## $ OfstedGrade : int 2 2 2 2 2 2 2 3 2 2 ... # Play with code nrow(SchoolData) ## [1] 58 # Play with code names(SchoolData) ## [1] &quot;SchoolName&quot; &quot;Type&quot; &quot;NumberBoys&quot; &quot;NumberGirls&quot; &quot;TotalStudents&quot; &quot;OfstedGrade&quot; You can examine specific variables from this data frame, for instance, we can get observation with the primary school that has the minimum and maximum number of students using the min() and max() function, respectively. We can also compute the total number primary students in Ealing using the sum(). We can compute these values using the $ symbol to select the column of interest, which is TotalStudents. Specify the name of the data frame (i.e., SchoolData) and then select the column of interest after the $ # maximum value max(SchoolData$TotalStudents) ## [1] 891 # minimum value min(SchoolData$TotalStudents) ## [1] 239 # total value sum(SchoolData$TotalStudents) ## [1] 29473 1.8.2 Basic subsetting and manipulation of data One can subset or restrict the data frame by specifying which row(s) and column(s) to keep or discard using this square bracket notation dataframe[Row, Column]. For example: # To select the 1st row of the first column in SchoolData SchoolData[1,1] ## [1] &quot;Berrymede Junior School&quot; # To select the first 5 rows of the first column SchoolData[1:5, 1] ## [1] &quot;Berrymede Junior School&quot; &quot;East Acton Primary School&quot; &quot;Oldfield Primary School&quot; &quot;North Ealing Primary School&quot; ## [5] &quot;St John&#39;s Primary School&quot; # To select the first 5 rows of the 1st, 5th and 6th column SchoolData[1:5, c(1,5:6)] ## SchoolName TotalStudents OfstedGrade ## 1 Berrymede Junior School 377 2 ## 2 East Acton Primary School 329 2 ## 3 Oldfield Primary School 465 2 ## 4 North Ealing Primary School 697 2 ## 5 St John&#39;s Primary School 471 2 # You can store sub-setted data into an object ReducedData &lt;- SchoolData[1:5, c(1,5:6)] Another trick one can do is use other columns or variables within a data frame to create another variable. This technique is essentially important when cleaning and managing data. From the data, we can derive proportion (%) of students who are boys for each school using $. Here, we can create a new column called PercentBoys as follows: # Create &#39;PercentBoys&#39; in the data frame &#39;SchoolData&#39; based on NumberBoys column and TotalStudents column SchoolData$PercentBoys &lt;- SchoolData$NumberBoys/SchoolData$TotalStudents * 100 1.8.3 Basic visualisation The school data only contains variables that are in counts or categories. Hence, we can only use visualisations such as pie charts or bar charts for such types of data (of course, in week 2, we’ll explore ways for visualising continuous data). The OfstedGrade variable has classified schools according to the overall performance from 1 (outstanding) to 4 (poor). We can visual the frequency of schools by performance grade in a bar chart using barchart() function: # tabulate the frequency of schools based on the OfstedGrade using table() counts &lt;- table(SchoolData$OfstedGrade) counts ## ## 1 2 3 4 ## 6 47 4 1 #Pass the counts into the barchart() barplot(counts) #Fully labelled Bar Plot barplot(counts, main=&quot;School Distribution based on OFSTED performance&quot;, xlab =&quot;OFSTED grades&quot;, ylab=&quot;Number of Schools&quot;) We can represent the above data again as a piechart reporting percentages instead using the pie() # tabulate the frequency of schools based on the OfstedGrade using table() counts &lt;- table(SchoolData$OfstedGrade) counts ## ## 1 2 3 4 ## 6 47 4 1 #Pass the counts into the pie() pie(counts) #Fully labelled Bar Plot pie(counts, main=&quot;Piechart on OFSTED performance&quot;, labels=c(&quot;Outstanding (1)&quot;, &quot;Good (2)&quot;, &quot;Not Good (3)&quot;, &quot;Bad (4)&quot;)) We can aggregate the number of students by performance to eyeball whether schools with more student perform either poorly or good on the OFSTED scale. # aggregate the frequency of students from schools based of OfstedGrade in a table using the tapply() totals &lt;- tapply(SchoolData$TotalStudents, SchoolData$OfstedGrade, FUN=sum) totals ## 1 2 3 4 ## 3210 24225 1692 346 #Fully labelled Bar Plot barplot(totals, main=&quot;Student Distribution based on OFSTED performance&quot;, xlab =&quot;OFSTED grades&quot;, ylab=&quot;Number of Primary Students in Ealing&quot;) The OfstedGrade variable has classified schools according to the overall performance from 1 (outstanding) to 4 (poor). We can visual the frequency of schools by performance grade in a bar chart using barchart() function: # tabulate the frequency of schools based on the OfstedGrade using table() counts &lt;- table(SchoolData$OfstedGrade) # Pass the counts to the barplot() function &amp; generate fully labelled Bar Plot barplot(counts, main=&quot;School Distribution based on OFSTED performance&quot;, xlab =&quot;OFSTED grades&quot;, ylab=&quot;Number of Schools&quot;) 1.8.4 Frequency Distributions A frequency distribution table is a nice to way to perform a simple univariable analysis to summarize the number of schools (frequency) that have a set number of students within certain fixed ranges. Let us gauge the minimum and maximum number of students, and create ranges between these two values. min(SchoolData$TotalStudents) ## [1] 239 max(SchoolData$TotalStudents) ## [1] 891 We are going to create a new ranges column from 200 (based from lowest value i.e., 239) to 900 (i.e., based on the highest which is 891) to look something like 200-300, 301-400, 401-500, …, 801-900 by cutting the TotalStudents using the cut() function. This will help us to create the desired frequency distribution table in RStudio. SchoolData$Ranges &lt;- cut(SchoolData$TotalStudents, breaks=c(200, 300, 400, 500, 600, 700, 800, 900), labels = c(&quot;200-300&quot;,&quot;301-400&quot;,&quot;401-500&quot;,&quot;501-600&quot;,&quot;601-700&quot;,&quot;701-800&quot;,&quot;801-900&quot;)) Now, lets create the frequency table that shows the following: (1.) the frequency of schools with student numbers in those ranges; (2.) Cumulative frequency of schools; and (3.) proportions of schools falling in those ranges. freqtable &lt;- table(SchoolData$Ranges) # see frequency of schools only freqtable ## ## 200-300 301-400 401-500 501-600 601-700 701-800 801-900 ## 5 8 20 11 7 4 3 # now, add the cumulative frequency and proportion to get table output in RStudio Output &lt;- transform(freqtable, CumuFreq = cumsum(Freq), Proportions=prop.table(Freq)) # see final output Output ## Var1 Freq CumuFreq Proportions ## 1 200-300 5 5 0.08620690 ## 2 301-400 8 13 0.13793103 ## 3 401-500 20 33 0.34482759 ## 4 501-600 11 44 0.18965517 ## 5 601-700 7 51 0.12068966 ## 6 701-800 4 55 0.06896552 ## 7 801-900 3 58 0.05172414 Above code shows you the rapid way of getting the frequency table. It only gives you the table though and there’s not decent way of getting the desired plots such as histogram and cumulative frequency plots because it involves of a lot of data wrangling… Here is the complete code - the R-way for do this analysis. Full Code Let’s create the appropriate graphical outputs to accompany the frequency table. Just like how we created the ranges from 200 to 900 (of intervals 100). We can also use the seq() function. This generates a sequence of numbers (i.e., 200, 300, 400, 500, 600, 700, 800, 900) which we can also create the groupings: # start at: 200 # end at: 900 # interval: 100 classes &lt;- seq(200, 900, 100) classes ## [1] 200 300 400 500 600 700 800 900 We are going to use the cut() function slice up the TotalStudents column accordingly into the bins or groupings we created from the seq(). This should categorise the schools in Ealing based on the number of students into groups of 200-300, 301-400, 401-500, …, 801-900. SchoolData$Groups &lt;- cut(SchoolData$TotalStudents, breaks=classes) SchoolData$Groups ## [1] (300,400] (300,400] (400,500] (600,700] (400,500] (400,500] (300,400] (500,600] (300,400] (200,300] (400,500] (400,500] (300,400] ## [14] (400,500] (600,700] (500,600] (500,600] (600,700] (400,500] (500,600] (200,300] (500,600] (300,400] (400,500] (700,800] (500,600] ## [27] (800,900] (800,900] (500,600] (400,500] ## [ reached getOption(&quot;max.print&quot;) -- omitted 28 entries ] ## Levels: (200,300] (300,400] (400,500] (500,600] (600,700] (700,800] (800,900] We going to generate the frequency table into a data frame and calculate components of our frequency table: i.e., frequency, relative frequency (or percentage), cumulative frequency and relative cumulative frequency all from the Group column: #this creates the frequency tables frequency_results &lt;- data.frame(table(SchoolData$Groups)) #this renames the columns colnames(frequency_results)[1] &lt;- &quot;Groups&quot; colnames(frequency_results)[2] &lt;- &quot;Frequency&quot; #this show the names of the columns names(frequency_results) ## [1] &quot;Groups&quot; &quot;Frequency&quot; # print frequency table frequency_results ## Groups Frequency ## 1 (200,300] 5 ## 2 (300,400] 8 ## 3 (400,500] 20 ## 4 (500,600] 11 ## 5 (600,700] 7 ## 6 (700,800] 4 ## 7 (800,900] 3 We got just the frequencies. Now, we to calculate the following: frequency, relative frequency (or percentage), cumulative frequency and relative cumulative frequency. The can be done as follows # calculating the relative freq., cumulative freq., and cumulative relative frequency frequency_results$RelativeFreq &lt;- frequency_results$Frequency/sum(frequency_results$Frequency) frequency_results$CumulativeFreq &lt;- cumsum(frequency_results$Frequency) frequency_results$CumulativeRelFreq &lt;- cumsum(frequency_results$RelativeFreq) Done! We can compute everything now… our complete frequency distribution table is: frequency_results ## Groups Frequency RelativeFreq CumulativeFreq CumulativeRelFreq ## 1 (200,300] 5 0.08620690 5 0.0862069 ## 2 (300,400] 8 0.13793103 13 0.2241379 ## 3 (400,500] 20 0.34482759 33 0.5689655 ## 4 (500,600] 11 0.18965517 44 0.7586207 ## 5 (600,700] 7 0.12068966 51 0.8793103 ## 6 (700,800] 4 0.06896552 55 0.9482759 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 1 rows ] For our histogram, we use the hist() function to make one. Let us plot this and again use the xlab=, ylab= and main= arguments to label the graph and axes appropriately. # histogram hist(SchoolData$TotalStudents, breaks = classes, ylab=&quot;Frequency for schools&quot;, xlab=&quot;Total students&quot;, main= &quot;Frequency Distribution of Schools in Ealing&quot;) Lastly, we create the cumulative frequency plots. # plot the cumulative frequency # extract the cumulative frequency column cumulative.freq0 &lt;- c(0, frequency_results$CumulativeFreq) cumulative.freq0 ## [1] 0 5 13 33 44 51 55 58 #create plot with classes values on x-axis and cumulative frequency values on y-axis plot(classes, cumulative.freq0, main=&quot;Cumulative Frequency Distribution of Schools in Ealing&quot; , xlab=&quot;Total Students&quot;, ylab=&quot;Cumulative Frequency: Schools&quot;) #connect the dots to each point lines(classes, cumulative.freq0) We also create the relative cumulative frequency plots. # plot for relative cumulative frequency which is a cumulative percentage # extract the relative cumulative frequency column rel.cumulative.freq0 &lt;- c(0, frequency_results$CumulativeRelFreq) rel.cumulative.freq0 ## [1] 0.0000000 0.0862069 0.2241379 0.5689655 0.7586207 0.8793103 0.9482759 1.0000000 #create plot with classes values on x-axis and cumulative frequency values on y-axis plot(classes, rel.cumulative.freq0, main=&quot;Relative Cumulative Frequency Distribution of Schools in Ealing&quot; , xlab=&quot;Total Students&quot;, ylab=&quot;Relative Cumulative Frequency [%]: Schools&quot;) #connect the dots to each point lines(classes, rel.cumulative.freq0) Interpretation: Out of the 58 primary schools in Ealing, 20 schools (0.3448 or 34.48%) have a student number totals ranging between 401-500 as the most frequent observation. The cumulative frequency of the range bracket for student numbers tells you how many schools have students from that range and lower. For instance 33 primary schools in Ealing have up to 500 students (which is 0.5689 or 56.89% of the data’s distribution). RECAP This conclude practical tutorials for week 1. You can save your script by pressing the save button. To recap, in this section you have learnt how to: Access R/RStudio Server and are familiar with RStudio’s environment Creating a data frame, and loading a CSV file into R using read.csv() Various basic functions for exploring the structure of a data frame, as well as how to subset data frame and manipulate columns to generate another. We explored various ways for produce basic plots for discrete and categorical variables using barplot() and pie() 1.9 Seminar tasks and questions Please find the seminar task and seminar questions for this week’s seminar below. Seminar Task : Use the seminar data set All Schools in London.csv and import it into RStudio. Perform the following tasks: Create a column called TotalStudents containing the total number of students for each school Create a column called PercentGirls which contains the estimated percentage of girls attending each school. Use the tapply() function to calculate the total number of student by subgroup or type of school Seminar Questions Answer for following questions: What are the names of schools that has the highest, and lowest number of students? [HINT: use max(), min() and View() functions] Generate a fully labelled barplot() and describe the distribution of schools by OFSTED scores. Explore the frequency of schools in London based on total number of students. Make a frequency table in R by creating the groups based on total students per school - starting from 0 to 2200, use the interval of 200 students to generate the groups as 0-200, 201-400, 401-600, …, 2001-2200 and hence number of schools falling into these groupings with these number of students. (Hint: see example in section 1.8.4. and make sure to use an appropriate interval for the ranges). Create a histogram. Create a cumulative frequency plot. Create a relative cumulative frequency plot. Provide an overall interpretation (see above example and those in Week 1’s lecture notes). Important Note: The solution codes for this week will be released by the end of Thursday. "],["examining-data-i.html", "Week 2 Examining data I 2.1 Introduction 2.2 Measures of central tendency 2.3 Simple plots 2.4 Measures of dispersion 2.5 Seminar task &amp; questions", " Week 2 Examining data I 2.1 Introduction Welcome to your second week of Introduction to Quantitative Research Methods. This week we will focus on examining data using measures of central tendency and measures of dispersion. These measures are collectively known as descriptive statistics. We will also talk about some basic data visualisation. Also, as by now, everyone should be up and running with RStudio Server, we will apply some of these descriptive measures onto some data. Alright, let’s get to it. 2.2 Measures of central tendency Any research project involving quantitative data should start with an exploration and examination of the available data sets. This applies both to data that you have collected yourself and data that you have acquired in a different way, e.g., through downloading official UK Census and labour market statistics. The set of techniques that is used to examine your data in first instance is called descriptive statistics. Descriptive statistics are used to describe the basic features of your data set and provide simple summaries about your data. Together with simple visual analysis, they form the basis of virtually every quantitative data analysis. 2.2.1 Accessing RStudio Server, uploading and importing data Make sure to download the data set for Week 2 from HERE if you have not already done so from the Welcome Page. The data sets are: For the tutorial practicals: London historical population dataset.csv For the seminar tasks and questions: Ambulance and Assault Incidents data.csv Let’s us first sign into RStudio Server: Log into the RStudio Server: https://rstudio.data-science.rc.ucl.ac.uk/. Log in with your usual UCL username and password. Create a new folder called Week 2 Upload the data London historical population dataset.csv into the folder of Week 2 Set the directory to the folder location of Week 2 to where the CSV file was uploaded by clicking on the More &gt;&gt; Set As Working Directory. Finally, import the data into workspace using the read.csv() function. # Load data into RStudio. The spreadsheet is stored in the object called &#39;London.Pop&#39; London.Pop &lt;- read.csv(&quot;London historical population dataset.csv&quot;) If you struggle with the above steps, as well as setting up your working directory. Then here is a reminder, have a look at how we did this last week! Use the head(), or View() functions to examine the structure of the London population data frame. The head() functions allows the users to see the first 5 rows of the data frame in the R-Console. The View() allows the user to see the entire spreadsheet in a data viewer. 2.2.2 Mean, Median and Mode We are going to use the following functions: mean() and median(), to compute the mean and median value respectively - these two estimates are one of the three measures of central tendency. We can apply these measures on our London.Pop data set. Let’s do this by focusing on London’s population in 2011. We going to keep the 1st, 2nd and 24th columns in the London.Pop data frame which corresponds to the columns called Area Code, Area Name and Person-2011, respectively. # select the relevant columns London.Pop2011 &lt;- London.Pop[,c(1:2,24)] Now we can calculate our measures of central tendency using R’s built-in functions for the median and the mean. # calculate the median of the 2011 population variable median(London.Pop2011$Persons.2011) ## [1] 254096 # calculate the mean of the 2011 population variable mean(London.Pop2011$Persons.2011) ## [1] 247695.2 Question(s) How do you explain that the median is larger than the mean for this variable? [HINT: What is distorting the mean value such that its brought the value down?] R does not have a standard in-built function to calculate the mode. As we still want to show the mode, we create a user function to calculate the mode of our data set. This function takes a numeric vector as input and gives the mode value as output. Note You do not have to worry about creating your own functions, so just copy and paste the code below to create the get_mode() function. # create a function to calculate the mode get_mode &lt;- function(x) { # get unique values of the input vector uniqv &lt;- unique(x) # select the values with the highest number of occurrences uniqv[which.max(tabulate(match(x, uniqv)))] } # calculate the mode of the 2011 population variable get_mode(London.Pop2011$Persons.2011) ## [1] 7375 Question(s) What is the level of measurement of our Persons.2011 variable? Nominal, ordinal, or interval/ratio? Even though we went through all the trouble to create our own function to calculate the mode, do you think it is a good choice to calculate the mode for this variable? Why? Why not? Although R does most of the hard work for us, especially with the mean() and the median() function, it is a good idea to once go through the calculations of these two central tendency measures ourselves. Let’s calculate the mean step-by-step and then verify our results with the results of R’s mean() function. # get the sum of all values Persons.2011.Sum &lt;- sum(London.Pop2011$Persons.2011) # inspect the result Persons.2011.Sum ## [1] 8173941 # get the total number of observations Persons.2011.Obs &lt;- length(London.Pop2011$Persons.2011) # inspect the result Persons.2011.Obs ## [1] 33 # calculate the mean Persons.2011.Mean &lt;- Persons.2011.Sum / Persons.2011.Obs # inspect the result Persons.2011.Mean ## [1] 247695.2 # compare our result with R&#39;s built-in function mean(London.Pop2011$Persons.2011) == Persons.2011.Mean ## [1] TRUE Great. Our own calculation of the mean is identical to R’s built-in function. Now let’s do the same for the median. # get the total number of observations Persons.2011.Obs &lt;- length(London.Pop2011$Persons.2011) # inspect the result Persons.2011.Obs ## [1] 33 # order our data from lowest to highest Persons.2011.Ordered &lt;- sort(London.Pop2011$Persons.2011, decreasing=FALSE) # inspect the result Persons.2011.Ordered ## [1] 7375 158649 160060 182493 185911 186990 190146 199693 206125 219396 220338 231997 237232 239056 246270 253957 254096 254557 254926 258249 ## [21] 273936 275885 278970 288283 303086 306995 307984 309392 311215 312466 ## [ reached getOption(&quot;max.print&quot;) -- omitted 3 entries ] # get the number of the observation that contains the median value Persons.Median.Obs &lt;- (Persons.2011.Obs + 1)/2 # inspect the result Persons.Median.Obs ## [1] 17 # get the median Persons.2011.Median &lt;- Persons.2011.Ordered[Persons.Median.Obs] # inspect the result Persons.2011.Median ## [1] 254096 # compare our result with R&#39;s built-in function median(London.Pop2011$Persons.2011) == Persons.2011.Median ## [1] TRUE 2.3 Simple plots Before moving on to the second set of descriptive statistics, the measures of dispersion, this is a good moment to note that simple data visualisations are also an extremely powerful tool to explore your data. In fact, tools to create high quality plots have become one of R’s greatest assets. This is a relatively recent development since the software has traditionally been focused on the statistics rather than visualisation. The standard installation of R has base graphic functionality built in to produce very simple plots. For example we can plot the relationship between the London population in 1811 and 1911. Note Next week we will be diving deeper into data visualisation and making plots and graphs in R, but for now it is a good idea to already take a sneak peek at how to create some basic plots. # make a quick plot of two variables of the London population data set plot(London.Pop$Persons.1811,London.Pop$Persons.1911) Experimenting with plot() What happens if you change the order of the variables you put in the plot() function? Why? Instead of using the $ to select the columns of our data set, how else can we get the same results? The result of calling the plot() function, is a very simple scatter graph. The plot() function offers a huge number of options for customisation. You can see them using the ?plot help pages and also the ?par help pages (par in this case is short for parameters). There are some examples below (note how the parameters come after specifying the x and y columns). # add a title, change point colour, change point size plot(London.Pop$Persons.1811, London.Pop$Persons.1911, main=&#39;Quick Plot in R&#39;, col=&#39;blue&#39;, cex=2) # add a title, change point colour, change point symbol plot(London.Pop$Persons.1811, London.Pop$Persons.1911, main=&quot;Another Quick Plot in R&quot;, col=&#39;magenta&#39;, pch=22) Also, you can apply titles to the axes using the xlab =\"\" and ylab=\"\" argument after the main=\"\" in the plot(). # add a axis titles, rotate number labels plot(London.Pop$Persons.1811, London.Pop$Persons.1911, main=&quot;Another Quick Plot in R&quot;, xlab = &quot;Population in 1811&quot;, ylab = &quot;Population in 1911&quot;, col=&#39;magenta&#39;, pch=22) You can prevent R from printing the scientific notation (e.g., 2e+05, 4e+05 etc.,) on the y-axis in the graphical output. You can turn it off by typing the following code before running the plot() function # turn-off horrible scientific notation options(scipen = 999) # add a axis titles, rotate number labels plot(London.Pop$Persons.1811, London.Pop$Persons.1911, main=&quot;Another Quick Plot in R&quot;, xlab = &quot;Population in 1811&quot;, ylab = &quot;Population in 1911&quot;, col=&#39;magenta&#39;, pch=22) Note For more information on the plot parameters (some have obscure names) have a look here: http://www.statmethods.net/advgraphs/parameters.html Recap In this section you have learnt how to: Calculate the mode, median, and mean of a variable in R. Make some simple scatter plots (in preparation for next week) to visualise your data. 2.4 Measures of dispersion When exploring your data, measures of central tendency alone are not enough as they only tell you what a ‘typical’ value looks like but they do not tell you anything about all other values. Therefore we also need to look at some measures of dispersion. Measures of dispersion describe the spread of data around a central value (e.g. the mean, the median, or the mode). The most commonly used measure of dispersion is the standard deviation. The standard deviation is a measure to summarise the spread of your data around the mean. Other measures of dispersion include the range, the interquartile range, and the variance. For the rest of this tutorial we will change our data set to one containing the number of assault incidents that ambulances have been called to in London between 2009 and 2011. You will need to download a prepared version of this file called: Ambulance and Assault Incidents data.csv and upload it to your working directory. It is in the same data format (csv) as our London population file so we use the read.csv() command again. # Load data into RStudio. The spreadsheet is stored in the object called &#39;London.Ambulance&#39; London.Ambulance &lt;- read.csv(&#39;Ambulance and Assault Incidents data.csv&#39;) # inspect the results head(London.Ambulance) ## BorCode WardName WardCode WardType Assault_09_11 ## 1 00AA Aldersgate 00AAFA Prospering Metropolitan 10 ## 2 00AA Aldgate 00AAFB Prospering Metropolitan 0 ## 3 00AA Bassishaw 00AAFC Prospering Metropolitan 0 ## 4 00AA Billingsgate 00AAFD Prospering Metropolitan 0 ## 5 00AA Bishopsgate 00AAFE Prospering Metropolitan 188 ## 6 00AA Bread Street 00AAFF Prospering Metropolitan 0 # inspect the number of rows and columns of the data set dim(London.Ambulance) ## [1] 649 5 You will notice that the data table has 5 columns and 649 rows. The column headings are abbreviations of the following: Column heading Full name Description BorCode Borough Code London has 32 Boroughs (such as Camden, Islington, Westminster, etc.) plus the City of London at the centre. These codes are used as a quick way of referring to them from official data sources. WardName Ward Name Boroughs can be broken into much smaller areas known as Wards. These are electoral districts and have existed in London for centuries. WardCode Ward Code A statistical code for the wards above. WardType Ward Type A classification that groups wards based on similar characteristics. Assault_09_11 Assault Incidents The number of assault incidents requiring an ambulance between 2009 and 2011 for each ward in London. Let’s start by calculating two measures of central tendency by using the median() and mean() functions. # calculate the median of the assault incident variable median(London.Ambulance$Assault_09_11) ## [1] 146 # calculate the mean of the assault incident variable mean(London.Ambulance$Assault_09_11) ## [1] 173.4669 Questions How do you explain that the mean is larger than the median for this variable? Great. Let’s now calculate some measures of dispersion for our data: the range, the interquartile range, and the standard deviation. The calculation of the range is very straightforward as we only need to subtract the the minimum value from the the maximum value. We can find these values by using the built-in min() and max() functions. # get the minimum value of the assault incident variable min(London.Ambulance$Assault_09_11) ## [1] 0 # get the maximum value of the assault incident variable max(London.Ambulance$Assault_09_11) ## [1] 1582 # calculate the range 1582 - 0 ## [1] 1582 # or in one go max(London.Ambulance$Assault_09_11) - min(London.Ambulance$Assault_09_11) ## [1] 1582 Questions What does this range mean? The interquartile range requires a little bit more work to be done as we now need to work out the values of the 25th and 75th percentile. Note A percentile is a score at or below which a given percentage of your data points fall. For example, the 50th percentile (also known as the median!) is the score at or below which 50% of the scores in the distribution may be found. # get the total number of observations London.Ambulance.Obs &lt;- length(London.Ambulance$Assault_09_11) # inspect the result London.Ambulance.Obs ## [1] 649 # order our data from lowest to highest London.Ambulance.Ordered &lt;- sort(London.Ambulance$Assault_09_11, decreasing=FALSE) # inspect the result London.Ambulance.Ordered ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 10 18 19 21 22 25 28 28 28 29 30 34 36 36 ## [ reached getOption(&quot;max.print&quot;) -- omitted 619 entries ] # get the number (&#39;index value&#39;) of the observation that contains the 25th percentile London.Ambulance.Q1 &lt;- (London.Ambulance.Obs + 1)/4 # inspect the result London.Ambulance.Q1 ## [1] 162.5 # get the number (&#39;index value&#39;) of the observation that contains the 75th percentile London.Ambulance.Q3 &lt;- 3*(London.Ambulance.Obs + 1)/4 # inspect the result London.Ambulance.Q3 ## [1] 487.5 # get the 25th percentile London.Ambulance.Ordered[163] ## [1] 86 # get the 75the percentile London.Ambulance.Ordered[488] ## [1] 233 # get the interquartile range 233 - 86 ## [1] 147 We can also visually represent our range, median, and interquartile range using a box and whisker plot: # make a quick boxplot of our assault incident variable boxplot(London.Ambulance$Assault_09_11, horizontal=TRUE) Questions There is a large difference between the range that we calculated and the interquartile range that we calculated. What does this mean? The 25th and 75th percentile in the example do not return integer but a fraction (i.e. 162.5 and 487.5). Why do we use 163 and 488 to extract our percentile values and not 162 and 487? Now, let’s move to the standard deviation. Remember: this is one of the most important measures of dispersion and is widely used in all kinds of statistics. The calculation involves the following steps: Calculate the mean. Subtract the mean from each observation to get a residual. Square each residual. Sum all residuals. Divide by \\(n-1\\). Take the square root of the final number. # calculate the mean London.Ambulance.Mean &lt;- mean(London.Ambulance$Assault_09_11) # subtract the mean from each observation London.Ambulance.Res &lt;- London.Ambulance$Assault_09_11 - London.Ambulance.Mean # square each residual London.Ambulance.Res.Sq &lt;- London.Ambulance.Res**2 # sum all squared residuals London.Ambulance.Res.Sum &lt;- sum(London.Ambulance.Res.Sq) # divide the sum of all sqaured residuals by n - 1 London.Ambulance.Variance &lt;- London.Ambulance.Res.Sum / (length(London.Ambulance$Assault_09_11) - 1) # take the square root of the final number London.Ambulance.Sd &lt;- sqrt(London.Ambulance.Variance) # standard deviation London.Ambulance.Sd ## [1] 130.3482 There we go. We got our standard deviation! You probably already saw this coming, but R does have some built-in functions to actually calculate these descriptive statistics for us: range(), IQR(), and sd() will do all the hard work for us! # range range(London.Ambulance$Assault_09_11) ## [1] 0 1582 # interquartile range IQR(London.Ambulance$Assault_09_11) ## [1] 147 # standard deviation sd(London.Ambulance$Assault_09_11) ## [1] 130.3482 Note Please be aware that the IQR() function may give slighlty different results in some cases when compared to a manual calculation. This is because the forumula that the IQR() function uses is slightly different than the formula that we have used in our manual calculation. It is noted in the documentation of the IQR() function that: “Note that this function computes the quartiles using the quantile function rather than following Tukey’s recommendations, i.e., IQR(x) = quantile(x, 3/4) - quantile(x, 1/4).” Questions What does it mean that we have a standard deviation of 130.3482? Given the context of the data, do you think this is a low or a high standard deviation? To make things even easier, R also has a summary() function that calculates a number of these routine statistics simultaneously. After running the summary() function on our assault incident variable, you should see you get the minimum (Min.) and maximum (Max.) values of the assault_09_11 column; its first (1st Qu.) and third (3rd Qu.) quartiles that comprise the interquartile range; its the mean and the median. The built-in R summary() function does not calculate the standard deviation. There are functions in other libraries that calculate more detailed descriptive statistics, # calculate the most common descriptive statistics for the assault incident variable summary(London.Ambulance$Assault_09_11) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 86.0 146.0 173.5 233.0 1582.0 Recap In this section you have learnt how to: Calculate the range, interquartile range, and standard deviation in R. Create a simple boxplot using the boxplot() function. Quickly get common descriptive statistics using the summary() function. 2.5 Seminar task &amp; questions Seminar Task: Use the seminar, you will be continuing with the data set Ambulance and Assault Incidents data.csv. Still work with the data frame object named as London.Ambulance. Create a new object / data set that only contains data for ward type Suburbs and Small Towns. Hint: Try to subset the data by filtering it based on WardType. Calculate the mode, median, mean, range, interquartile range, and standard deviation for the Assault_09_11 variable for Suburbs and Small Towns Produce a boxplot() that provides a visual description of it’s distribution Seminar questions Compare the results of the descriptive statistics you have calculated for your Suburbs and Small Towns object with the results of the descriptive statistics you have calculated for you Prospering Metropolitan object / data set. What do these differences tell us about the levels of violent assaults within these separate environments? Create a dual boxplot to show a visual representation of this comparison. Important Note: The solution codes will be released next week on Monday. Before you leave, do save your R script by pressing the Save button in the script window. "],["examining-data-ii.html", "Week 3 Examining data II 3.1 Introduction 3.2 Data visualisation 3.3 ggplot2 3.4 Seminar 3.5 Before you leave", " Week 3 Examining data II 3.1 Introduction Welcome to your third week of Introduction to Quantitative Research Methods. This week we will focus again on examining data, however, this time we will focus on data visualisation. For the tutorial we will reuse both data sets that we used over the last week. Alright, let’s go and have some fun. 3.2 Data visualisation Data visualisation is the representation of data in a visual format. This could be a graph, chart, map, or other visual format. Sometimes the visualisation includes every data point, for instance in case of a scatter graph, or sometimes it shows some type of statistical summary, for instance in case of a boxplot. Sometimes visualisations are very descriptive and only include the raw data, whilst other times they are the product of a sequence of transformations and analyses. No matter what, the main goal of data visualisation is to help you interpret the underlying data. Data visualisation is therefore used when cleaning your data, exploring your data, looking for outliers and unusual values, identifying trends, identifying clusters, uncovering patterns, and presenting results. Before we move to R and start creating some data visualisations ourselves, a good example, albeit slightly dated, of why data visualisation is so important, is given by the late Hans Rosling. He was a Swedish physician and statistician who was very passionate about data visualisation. As he says in the video: “Having the data is not enough. I have to show it in ways people both enjoy and understand”. Video: Data visualisation II [Watch on YouTube] For this tutorial we start by using the London.Pop object again that we created during last week’s tutorial. You may still have it loaded into your R workspace if your RStudio Server session is still active. At this point, you should be able to set your RStudio Server, upload data to it and set the directory etc., if you struggle with setting up your working directory and you need a reminder, then have a look at how we set this up in our first week! # Load data into RStudio. The spreadsheet is stored in the object called &#39;London.Pop&#39; London.Pop &lt;- read.csv(&quot;London historical population dataset.csv&quot;) Now are data loaded again, we can start by recreating a simple scatter graph in a similar fashion as we did last week using the plot() function. # make a quick scatter plot of two variables of the London population data set plot(London.Pop$Persons.2001,London.Pop$Persons.2011) Ask yourself these questions What do the points in the graph represent? Do you think there is a relationship between the population in London’s boroughs in 2001 and the population in London’s boroughs in 2011? If so, what is this relationship? How can we add a title? How can we change the colour of the points? Another common method to visualise your data is by using a line chart. Line charts are particularly useful when you want to compare change over time, for example. A line chart can also be relatively easily created in R. Let’s try it by charting the population size over time for one of London’s boroughs: Hackney. # select the data for the borough of Hackney London.Hackney &lt;- London.Pop[London.Pop$Area.Name==&#39;Hackney&#39;,3:24] # inspect the result London.Hackney ## Persons.1801 Persons.1811 Persons.1821 Persons.1831 Persons.1841 Persons.1851 Persons.1861 Persons.1871 Persons.1881 Persons.1891 Persons.1901 ## 12 50000 64000 80000 105000 128000 170000 218000 261000 329000 372000 389000 ## Persons.1911 Persons.1921 Persons.1931 Persons.1939 Persons.1951 Persons.1961 Persons.1971 Persons.1981 Persons.1991 Persons.2001 Persons.2011 ## 12 385000 379000 364000 332000 265000 257522 220000 180434 162772 202825 246270 The crucial part of the code snippet above is what’s included in the square brackets [ ]. We are subsetting the London.Pop object, but instead of telling R what column names or numbers we require, we are requesting all rows in the Area.Name column that contain Hackney. Hackney is a text string so it needs to go in speech marks '' (or \"\") and we need to use two equals signs == in R to mean “identical to”. A single equals sign = is another way of assigning objects or saying “equal to”. Although = works the same way as &lt;-, it is much less widely used for this purpose because it is also used when parameterising functions. We should now be left with a dataframe that only contains one row of data: the data for the borough of Hackney. This does raise a question: how do we include all the variables into our plot as all our data are spread out over different columns (i.e. Persons1801, Persons1811, etc.)? Ideally we would flip the data set around so that all the data (i.e. Persons1801, Persons1811, etc.) would be in one column. Well, luckily we can actually do that by transposing our data set. Transposing is a mathematical operation to flip a matrix over its diagonal. Effectively, it turns rows into columns and columns into rows. In R we can do this using the t() function. # flip the rows and columns of our data set London.Hackney.t &lt;- t(London.Hackney) # inspect the result London.Hackney.t ## 12 ## Persons.1801 50000 ## Persons.1811 64000 ## Persons.1821 80000 ## Persons.1831 105000 ## Persons.1841 128000 ## Persons.1851 170000 ## Persons.1861 218000 ## Persons.1871 261000 ## Persons.1881 329000 ## Persons.1891 372000 ## Persons.1901 389000 ## Persons.1911 385000 ## Persons.1921 379000 ## Persons.1931 364000 ## Persons.1939 332000 ## Persons.1951 265000 ## Persons.1961 257522 ## Persons.1971 220000 ## Persons.1981 180434 ## Persons.1991 162772 ## Persons.2001 202825 ## Persons.2011 246270 # plot the size of the population in the London borough of Hackney over time plot(London.Hackney.t, type=&#39;b&#39;) Ask yourself these questions Why did we slice our data frame when we selected the data for Hackney (i.e. London.Pop[London.Pop$Area.Name=='Hackney',3:24]) and did we not simply keep all columns? How can we change the colour of the line and points in our graph? Note You may be wondering why the first column of the transposed dataframe does not have a column name and why the column with the data of the transposed dataframe is called 12. The reason for this is that the t() function flips the row and column indices around. The column indices (i.e. in this case the column names) now become row names and the row indices (i.e. in this case row numbers) now become column names (the data for Hackney can be found on row number 12 in the original data set). However, do not worry about this too much for now. Last week you also got introduced to the box and whisker plot. A box and whisker plot is a method for visualising your data by their quartiles with the whiskers (the lines extending from the box) incidating the variability of the values outside the lower and upper quartile. Typically outliers are plotted as individual points. Let’s go back to our full data set and make a boxplot for one of the population variables in the London population data set. # make a boxplot of one of the variables in the London population data set boxplot(London.Pop$Persons.2001,horizontal=TRUE) The standard plot() and boxplot functions offer a huge number of options for customisation. A useful option is to combine multiple graphs into one figure to allow for easy comparison. We can do this by setting some of the graphical parameters and requesting that our graphs are plotted together. # specify that we want an output consisting of two rows par(mfrow=c(2,1)) # make a boxpot of one of the variables in the London population data set boxplot(London.Pop$Persons.2001,horizontal=TRUE) # make a boxplot of another variable in the London population data set boxplot(London.Pop$Persons.2011,horizontal=TRUE) Note The specification given to the par() function only works once: it is not an option that you switch on and once the option is switched on all plots will be plotted together. This also means that if you run this code line by line, you will not get the result you want. In order to get both boxplots to show in one figure, you need to highlight all three lines and then run the code. Ask yourself these questions The boxplots of the population in London’s boroughs in 2001 and the population in London’s boroughs in 2011 are clearly different. What differences are there and what does this mean? Why is it currently difficult to compare these two boxplots? What happens if we update our parameters to par(mfrow=c(1,2))? The last type of plot we will introduce to you today is called a histogram. As explained in the lecture video, a histogram is a graphical display of data using bars of different heights. It is therefore quite similar to a bar chart, however, with a histogram we group our values into bins. We can create in histogram in R using the hist() function. # make a histogram of one of the variables in the London population data set hist(London.Pop$Persons.2001) Currently R automatically picks the width of the bins in which the data are grouped. To change the width of these bins, we can specify the breaks parameter: # create a vector with break values bindwidth &lt;- c(0,100000,200000,300000,400000) # make a histogram of one of the variables in the London population data set hist(London.Pop$Persons.2001,breaks=bindwidth) Recap In this section you have learnt how to: Create a simple line chart in R. Combine multiple graphs into one figure. Create a histogram in R. 3.3 ggplot2 The graphs and figures we have made so far are not really pretty. Although possible with the basic R installation, there are easier and better ways to make nice visualisations. For this we can turn to other R packages that have been developed. In fact, there are many hundreds of packages in R each designed for a specific purpose, some of which you can use to create plots in R. One of those packages is called ggplot2. The ggplot2 package is an implementation of the Grammar of Graphics (Wilkinson 2005) - a general scheme for data visualisation that breaks up graphs into semantic components such as scales and layers. ggplot2 can serve as a replacement for the base graphics in R and contains a number of default options that match good visualisation practice. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. As there are many hundreds of R packages, these are not installed automatically. This means that every additional package we want to use needs to be downloaded and then we need to load it into R in order to use it. To download and install the ggplot2 package type the following: # install package install.packages(&quot;ggplot2&quot;) Note If you are running RStudio on your own computer: when you hit ‘enter’ you could be asked to select a mirror to download the package contents from. It does not really matter which one you choose, but we would suggest you pick the mirror that is geographically closest to you. The install.packages() step only needs to be performed once. You do not need to install a the package every time you want to use it. However, each time you open R, or start a new R session in RStudio Server, and wish to use a package you need to use the library() command to tell R that it will be required. # load ggplot2 package without quotes library(ggplot2) Now we installed ggplot2 and loaded it into our R session, we can start by trying to make a basic plot using the ggplot2 package ourselves. Let’s continue for a little while longer with our London.Pop object. # create a ggplot2 object named &#39;p&#39; p &lt;- ggplot(data=London.Pop, aes(Persons.1811, Persons.1911)) What you have just done is set up a ggplot object where you tell where you want the input data to come from – in this case it is our London.Pop object. The column headings within the aes() brackets refer to the parts of that data frame you wish to use (the variables Persons.1811 and Persons.1911). aes is short for aesthetics that vary – this is a complicated way of saying the data variables used in the plot. Let’s have a look at this object. # plot p As you can see, if you inspect the object p that we just created you will get an empty canvas. The reason for this is that you have not told ggplot what you want to do with the data. We do this by adding so-called geoms, geometries. Let’s try to create a scatter plot. We can do this by using the geom geom_point(): # add our geom to our &#39;p&#39; object p &lt;- p + geom_point() # plot p You can already see that this plot is looking a bit nicer than the scatterplot we created with the base plot() function used above. Within the geom_point() brackets you can alter the appearance of the points in the plot. Questions Try to change the colour and size of the points in your scatterplot by specifying the colour and size parameter, for instance, p + geom_point(colour='red', size=2). What other parameters can you pass to the geom_point() function? Note Whilst these instructions are step by step, you are strongly encouraged to deviate from them, for instance, by trying different colours, to get a better understanding of what we are doing. For further help, ggplot2 is one of the best documented packages in R and large volumes of documentation are available. If you want to explore ggplot2 further, we would also recommend taking a look at the ggplot2 tutorial for beautiful plotting in R by by Cédric Scherer. Rather than colouring your points by one colour, you can also colour the points according to another variable. You can do this by adding the desired variable into the aes() section after geom_point(). Here we will do this to indicate the size of the population in 2011 as well as the relationship between the size of the population in 1811 and 1911. # add some more aesthetics that vary p + geom_point(aes(colour = Persons.2011), size = 2) You will notice that ggplot has also created a key that shows the values associated with each colour. In this slightly contrived example it is also possible to resize each of the points according to the Persons.2011 variable. # add some more aesthetics that vary p + geom_point(aes(size = Persons.2011)) The real power of ggplot2 lies in its ability to build a plot up as a series of layers. This is done by stringing plot functions (geoms) together with the + sign. For instance, we can add a text layer to the plot using geom_text(). Note This idea of layers (or geoms) is quite different from the standard plot functions in R, but you will find that each of the functions does a lot of clever stuff to make plotting much easier (see the ggplot2 documentation for a full list). # add a geom_text to the plot p + geom_point(aes(size = Persons.2011)) + geom_text(size = 2, colour=&#39;red&#39;, aes(label = Area.Name)) The above code adds London Borough labels to the plot over the points they correspond to. This is not perfect since many of the labels overlap but they serve as a useful illustration of the layers. To make things a little easier the plot can be saved as a pdf using the ggsave() command. When saving the plot can be enlarged using the scale parameter to help make the labels more legible. # save the plot ggsave(&#39;first_ggplot.pdf&#39;, scale=2) Ask yourself this question Where does your plot get saved? Why? Please note that ggsave() only works with plots that were created with ggplot. Within the brackets you specify the file name for the plot, but as you can see in the example you also include the file format: in this case .pdf, but you could also save the plot as a .jpg file. The scale controls how many times bigger you want the exported plot to be than it currently is in the plot window. ggplot2 is not limited to scatterplots and we can also create more advanced plots such as histograms and box and whisker plots. Let’s first switch our data set to the one containing the number of assault incidents that ambulances have been called to in London between 2009 and 2011. As you will already have uploaded this Ambulance and Assault Incidents data.csv file to your working directory last week, we can simply reload the file if it is not still loaded into your R workspace. # Load data into RStudio. The spreadsheet is stored in the object called &#39;London.Ambulance&#39; London.Ambulance &lt;- read.csv(&#39;Ambulance and Assault Incidents data.csv&#39;) Histograms provide a nice way of graphically summarising a dataset, so let’s start by making a histogram for our assault incident variable. # create a ggplot2 object named &#39;p&#39; p &lt;- ggplot(London.Ambulance, aes(x=Assault_09_11)) # inspect p The ggplot(London.Ambulance, aes(x=Assault_09_11)) section means “create a generic plot object (called p) from the input object using the Assault_09_11 column as the data for the x axis”. Remember the data variables are required as aesthetics parameters so the Assault_09_11 appears in the aes() brackets. To create the histogram you need to add the relevant ggplot2 command (geom). # add geom to our &#39;p&#39; object p + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The height of each bar (the x-axis) shows the count of the datapoints and the width of each bar is the value range of datapoints included. If you want the bars to be thinner (to represent a narrower range of values and capture some more of the variation in the distribution) you can adjust the bin width. Bin width controls the size of ‘bins’ that the data are split up into. Try: # add geom to our &#39;p&#39; object, adjust the bin width settings p + geom_histogram(binwidth=10) This plot has provided a good impression of the overall distribution, but it would be interesting to see characteristics of the data within each of the Boroughs. We can do this since each Borough in the London.Ambulance object is made up of multiple wards. To see what we mean, we can select all the wards that fall within the Borough of Camden, which has the code 00AG (if you want to see what each Borough the code corresponds to, and learn a little more about the statistical geography of England and Wales, then do have a look here. # filter the data set camden &lt;- London.Ambulance[London.Ambulance$BorCode==&#39;00AG&#39;,] Just like before, the crucial part of the code snippet above is what’s included in the square brackets. Again we are subsetting the London.Ambulance object, but instead of telling R what column names or numbers we require, we are requesting all rows in the BorCode column that contain 00AG. Let’s quickly compare our original London.Pop object with our newly created camden object: # inspect the ambulance assault incident data set nrow(London.Pop) ## [1] 33 # inspect the ambulance assault incident data set for Camden nrow(camden) ## [1] 18 # inspect the ambulance assault incident data set for Camden head(camden) ## BorCode WardName WardCode WardType Assault_09_11 ## 128 00AG Belsize 00AGGD Prospering Metropolitan 91 ## 129 00AG Bloomsbury 00AGGE Prospering Metropolitan 315 ## 130 00AG Camden Town with Primrose Hill 00AGGF Prospering Metropolitan 535 ## 131 00AG Cantelowes 00AGGG Multicultural Metropolitan 238 ## 132 00AG Fortune Green 00AGGH Prospering Metropolitan 106 ## 133 00AG Frognal and Fitzjohns 00AGGJ Prospering Metropolitan 77 Questions Why do we have to use double quotes ('') to subset out dataframe to get all rows that contain 00AG? So to produce a histogram for Camden, the code above needs to be replicated using the camden object in the place of London.Pop: # create a ggplot2 object named &#39;p.camden&#39; p.camden &lt;- ggplot(camden, aes(x=Assault_09_11)) # add geom to our &#39;p.camden&#39; object p.camden + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # plot pretty(ish) p.camden + geom_histogram() + ggtitle(&#39;Assault incidents in Camden&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Questions What do the values on the x-axis of our histogram mean? What do the values on the y-axis of our histogram mean? As you can see this histogram looks a little different than the histogram for the entire data set did. This is largely because we have relatively few rows of data in the camden object (as we saw when using nrow(camden)). Nevertheless it would be interesting to see the data distributions for each of the London Boroughs. It is a chance to use the facet_wrap() function in R. This brilliant function lets you create a whole load of graphs at once! # note that we are back to using the `p` object since we need all our data for this # this code may generate a large number of warning messages relating to the plot bin width, don&#39;t worry about them p + geom_histogram() + facet_wrap(~BorCode) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Yes. It is that easy. Let’s try using facet_wrap() to plot according to Ward type: # note that we are back to using the `p` object since we need all our data for this # this code may generate a large number of warning messages relating to the plot bin width, don&#39;t worry about them p + geom_histogram() + facet_wrap(~WardType) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The facet_wrap() part of the code simply needs the name of the column you would like to use to subset the data into individual plots. Before the column name a tilde ~ is used as shorthand for “by” - so using the function we are asking R to facet the input object into lots of smaller plots based on the BorCode column in the first example and WardType in the second. Questions Use the facet_wrap() help file to learn how to create the same plot but with the graphs arranged into 4 columns. Which parameters need to be specified? In addition to histograms, another type of plot that, as we know, shows the core characteristics of the distribution of values within a data set is a box and whisker plot. These too can be easily produced using the ggplot2 package: # note that the `Assault_09_11` column is now y and not x # we specify x = &#39;London&#39; to add a meaningfull label to the x-axis p.boxplot &lt;- ggplot(London.Ambulance, aes(x=&#39;London&#39;, y=Assault_09_11)) # add the boxplot geom p.boxplot + geom_boxplot() If we are just interested in Camden then we can use the camden object created above in the code. # boxplot for camden only # we speficy x = &#39;Camden&#39; to add a meaningfull label to the x-axis p.boxplot.camden &lt;- ggplot(camden, aes(x=&#39;Camden&#39;, y=Assault_09_11)) # add the boxplot geom p.boxplot.camden + geom_boxplot() If you prefer you can flip the plot 90 degrees so that it reads from left to right: # add the boxplot geom and rotate p.boxplot.camden + geom_boxplot() + coord_flip() You can see that Camden looks a little different to the boxplot of the entire dataset. It would therefore be useful to compare the distributions of data within each of the Boroughs in a single plot as we did with the histograms above. ggplot2 makes this very easy (again!), we just need to change the x parameter to the Borough code column (BorCode). # boxplot for camden only p.boxplot.all &lt;- ggplot(London.Ambulance, aes(x=BorCode, y=Assault_09_11)) # add the boxplot geom and rotate p.boxplot.all + geom_boxplot() + coord_flip() Recap In this section you have learnt how to: Install and load additional packages in R. Learned the basics of the ggplot2 package for creating plots. Learned what geoms are in the context of ggplot2. Learned how to specify data variables with the aes() parameter. Utilised some of the advanced functionality as part of the ggplot2 package, not least through the creation of facetted histogram plots using geom_histogram() and facet_wrap() and also box and whisker plots with geom_boxplot(). 3.4 Seminar Please find the seminar task and seminar questions for this week’s seminar below. Note Please make sure that you have executed the seminar task and have answered the seminar questions before the seminar! Seminar task The goal of this task is to help you know how to use ggplot() function to create graphs, as well as to knowing how to apply the basic customisations to such ggplot() graphs. The most important customisations you should know, at least, are: how to add titles, apply labelling to x-y axis, changing the background’s theme (i.e., colour), and making some cosmetic changes to the colours on the actual plot. Lastly, knowing how to export your graph as a .png format. Let us start Create a histogram of the Assault_09_11 variable for the London borough of Ealing (Borough code 00AJ), you need to: Use the ggplot2 library. Use geom_histogram(). Use a bin width of 75. Add a title using ggtitle(). Add a title to x-axis using xlab() Add a title to y-axis using ylab() Now apply further customisation to the histogram you created. Try to add the following: Change the background’s theme using theme_classic() for image to have a conservative appearance Use a bin width of 100 instead of 75 Change the fill colour of the histogram’s bars to white Change the colour of the histogram’s bars’ outline to black Change the size for the title to 8 using theme(plot.title = element_text(size=)). Customise both the x and y-axis using size of 8 by adding axis.text=element_text(size=) and axis.title=element_text(size=,face=\"bold\") in the theme() function. Give the main, x and y-axis titles a bold face Save your histogram as a .png using ggsave() (note it’s possible to save as .jpg and .pdf. But .png is better). Use the following setting: width and height is 9cm. The units’ settings in cm and dpi set to 300. Some hints: The binwidths, and cosmetic changes (colour, fill etc) to histogram are specified in geom_histogram() function Use ggtitle() to add title alongside with theme(plot.title = element_text(size=)) to control the size of title. I personally use the size of 8 all the time as title size comes out reasonable (not too large nor small). The axis.title= and axis.text customise the labelling on the axis accordingly. They must be mentioned in the theme() function. Use ggsave() to save image with the following options: the width &amp; height options controls the size of image, units are in pixels px (or centimeters cm), and the dpi i.e., (dots per inch) controls the image quality. The best and standard specification is 300 or above - in short, just keep the settings at 300 as the setting i.e.,dpi=300 to avoid any grief. Now, create a boxplot of the Assault_09_11 variable for Ealing - now try the following: Use the ggplot2 library. Use geom_boxplot(). It should appear vertical Add a title using ggtitle(). Apply the customisation to your boxplot accordingly and experiment. Save your boxplot as a .png. Seminar questions Explain why each of these visualisations are useful and what type of data are required to calculate them: Scatter plot Histogram Box and whisk plot Line chart Recreate and compare the results of the histograms you created of the Assault_09_11 variable for the London borough of Croydon (Borough code 00AH) when using a bin width of 50 versus a bin width of 200. Have the two plots side-by-side (see hint). Why do they look so different? What does this tell you about the selection of bin widths? Hints: The gridExtra package let’s you combine multiple plots created from ggplot(). Install this package using install.packages() function and use library() to active it. gridExtra gives you access to a function called grid.arrange(). Specify the number of columns as 2 using ncol=2 and rows as 1 nrow=1 the images to appear side-by-side. Make sure to alternate the size of the text to avoid things getting all squished up. Remember, this is your go-to function to plot images side-by-side. Exporting as .png make the width longer to some sensible dimension again to avoid the plots looking squished. 3.5 Before you leave Save your R script by pressing the Save button in the script window. "],["sourcing-data.html", "Week 4 Sourcing data 4.1 Introduction 4.2 Sourcing data 4.3 Preparing data 4.4 Seminar", " Week 4 Sourcing data 4.1 Introduction Welcome to your fourth week of Introduction to Quantitative Research Methods. This week we will introduce you to sourcing and preparing data from official sources. For the tutorial we will apply what we have learnt over the past weeks onto a new secondary data set. OK… one final push to wrap the basic stuff covered today and previous weeks. 4.2 Sourcing data Over the past weeks we have predominantly worked with two data sets that we provided for you: Ambulance and Assault Incidents data.csv and London historical population dataset.csv. Although these data sets are very useful to give you an introduction to Quantitative Research methods, in real life, you will probably have to source your own data. In the case of the social sciences, you often can find socioeconomic data on the websites of national statistical authorities such as the Office for National Statistics, United States Census Bureau, Statistics South Africa, or the National Bureau of Statistics of China. Also large international institutions like World Bank and Unicef compile socioeconomic data sets and make these available for research purposes. No matter where you will be getting your data from, however, you will have to know how to download and prepare these data sets in such a way that you can work with them in R to conduct your analysis. Within the United Kingdom, the Office for National Statistics (ONS) is the largest producer of official statistics. ONS is responsible for collecting and publishing statistics related to the economy, population and society at national, regional and local levels. They are also responsible for conducting the census in England and Wales every 10 years (the census for Scotland and Northern Ireland are conducted by the National Records of Scotland and the Northern Ireland Statistics and Research Agency, respectively). Today we will be downloading a data set from ONS, prepare the data set as a csv file, and read our freshly created csv file into R. Figure 4.1: The website of the Office for National Statistics. Because the population data in the London historical population dataset.csv we have been working with so far only goes as far as 2011, we will try to get some more recent population estimates on the ‘usual resident population’. Every year, ONS releases a new set of Middle Super Output Area mid-year population estimates. Currently, the latest available data set is that of mid-2019 and this is the data set that we are now going to download and prepare. Note The mid-year population estimates that we will be downloading are provided by ONS at the Middle Super Output Area (MSOA) level. MSOAs are one of the many administrative geographies that the ONS uses for reporting their small area statistics. An administrative geography is a way of dividing the country into smaller sub-divisions or areas that correspond with the area of responsibility of local authorities and government bodies. These geographies are updated as populations evolve and as a result, the boundaries of the administrative geographies are subject to either periodic or occasional change. The UK has quite a complex administrative geography, particularly due to having several countries within one overriding administration and then multiple ways of dividing the countries according to specific applications. To download the data set, you need to take the following steps: Step Action 1 Navigate to the download page of the Middle Super Output Area population estimates: [Link] 2 Download the file Mid-2020: SAPE23DT4 to your computer. 3 Because the file that you have now downloaded is a zip file, we first need to extract the file before we can use it. To unzip the file you can use the built-in functionality of your computer’s operating system. For Windows: right click on the zip file, select Extract All, and then follow the instructions. For Mac OS: double-click the zip file. 4 Open the file in Microsoft Excel or any other spreadsheet software. Please note that the instructions provided below use only cover Microsoft Excel. 5 Once opened your file should look similar as the screenshot in Figure 4.2. Figure 4.2: The Mid-2020: SAPE23DT4 file that we downloaded. The download file will be in proper excel format .xlsx. The file name will look annoyingly look finicky: sape23dt4mid2020msoasyoaestimatesunformatted.xlsx. This is typical of an ONS data. The file probably does not look exactly like you thought it would because we do not directly see any data! In fact, ONS has put the data on different tabs. Some of these tabs contain the actual data, whilst others contain some meta-data and notes and definitions. The data that we want to use is found on the Mid-2020 Persons tab: the total number of people living in each MSOA. The problem we have now, however, is that the data as it is right now cannot be read into R without causing us lots of problems down the road. So even though we have the data we want to work with at our finger tips, we are not yet ready to read our data set into R! Follow the instructions for cleaning the data set manually. These are the steps that you need to take to turn the downloaded data into a csv: Step Action 1 Open the downloaded file in Microsoft Excel and activate the Mid-2020 Persons tab. 2 Highlight the columns: MSOA Code, MSOA Name, LA Code (2018 boundaries), LA name (2018 boundaries), LA Code (2020 boundaries), LA name (2020 boundaries), and All Ages. Do make sure that you do not include any of the whitespace or empty rows. 3 Scroll down all the way to the bottom of the file. Now hold down the shift button and click on the last value in the All Ages column. This value should be 9,722. All data should now be selected. 4 Now all the data that we need are selected we can copy them by right clicking and in the context menu opting for the copy option. Of course, you can also simply use control + c (Windows) or command + c (Mac OS). 5 Open a new, empty spreadsheet and paste the copied data into this new, empty spreadsheet. You can paste your copied data by right clicking on the first cell (A1) and in the context menu opting for the paste option. Of course, you can also simply use control + v (Windows) or command + v (Mac OS). 6 Conduct a visual check to make sure that you copied all the data. 7 Conduct a visual check to make sure that you did not copy any additional data. 8 Remove all formatting and make sure that empty columns are indeed empty. 9 Save this file as a midyear2020.csv. Make sure that you select csv as your file format (e.g. CSV UTF8 (Comma-delimited) (.csv)). 10 Inspect your data in a text editor such as Wordpad or Textedit to make sure the file you created is indeed a comma-separated file. Now, for the moment of truth: let’s try and see if we can load our data into R! Of course, you will first need to upload your csv file to RStudio Server and set your working directory so that R can find the file - but you should be able to do that without any problems by now. # load csv file from working directory midyear &lt;- read.csv(&#39;midyear2020.csv&#39;) # inspect midyear Questions Inspect the column names. Why do you think that they slightly differ from the ones that we saw in Microsoft Excel? How many rows does the midyear dataframe have? How many columns does the midyear dataframe have? The column names may seem a little complicated but in fact simply refer to some of the administrative geographies that the ONS uses for their statistics. Don’t worry if you do not fully understand these as they are notoriously complicated and confusing! Column heading Full name Description MSOA.Code MSOA Code Middle Super Output Areas are one of the many administrative geographies that the ONS uses for reporting their small area statistics. These codes are used as a quick way of referring to them from official data sources. MSOA.Name MSOA Name Name of the MSOA. LA.Code..2018.boundaries Local Authority District codes 2018 Local Authority Districts are a subnational division used for the purposes of local government. Each MSOA belongs to one Local Authority District, however, between years the boundaries of these Local Authority Districts sometimes change. These are the codes of the Local Authority District to which the MSOA belonged to in 2018. LA.name..2018.boundaries Local Authority District names 2018 Name of the Local Authority District to which the MSOA belonged to in 2018. LA.Code..2020.boundaries Local Authority District codes 2020 Code of the Local Authority District to which the MSOA belonged to in 2020 LA.name..2020.boundaries Local Authority District names 2020 Name of the Local Authority District to which the MSOA belonged to in 2020. All.Ages Total number of people The total number of people that is estimated to live in the MSOA mid-2020. Now we at least have an idea about what our column names mean, we can rename them so that they are a little more intelligible and easier to work with in a later stage. # rename columns names(midyear) &lt;- c(&#39;msoa_code&#39;,&#39;msoa_name&#39;,&#39;lad18_code&#39;,&#39;lad18_name&#39;,&#39;lad20_code&#39;,&#39;lad20_name&#39;,&#39;pop20&#39;) # inspect names(midyear) ## [1] &quot;msoa_code&quot; &quot;msoa_name&quot; &quot;lad18_code&quot; &quot;lad18_name&quot; &quot;lad20_code&quot; &quot;lad20_name&quot; &quot;pop20&quot; The write.csv() is the command we use to save our data. # write to new csv, specify the object to be export out as a csv &#39;midyear&#39; object # next make sure to type the new name of the csv file to be named as: i.e., &#39;midyear2020_v2.csv&#39; write.csv(midyear, &#39;midyear2020_v2.csv&#39;, row.names=FALSE) Great. This is all looking very good but we are not completely there yet. Now we sourced our data and managed to successfully read the data set into R, we will need to do conduct some final preparations so that our data is analysis ready. Recap In this section you have learnt how to: Download data from the Office for National Statistics. Select the data that you need and save these into a csv file. Load the csv file that you created into R. 4.3 Preparing data Even though the data set that we downloaded came from an official national statistical authority, the data is not directly ready for analysis. In fact, the vast majority of the data you will find in the public domain (or private domain for that matter) will be dirty data. With dirty data we mean data that needs some level of pre-processing, cleaning, and linkage before you can use it for your analysis. In the following, you will learn a consistent way to structure your data in R: tidy data. Tidy data, as formalised by R expert Hadley Wickham in his contribution to the Journal of Statistical Software is not only very much at the core of the tidyverse R package that we will introduce you to, but also of general importance when organising your data. As you know by now, your basic R functionality (the base package) can be extended by installing additional packages. These packages are the fundamental units of reproducible R code and include functions, documentation, and sample data. Last week, we introduced you to the ggplot2 package: a general scheme for data visualisation that breaks up graphs into semantic components such as scales and layers. Today we will introduce you to the tidyverse package. This package is specifically created to make working with data, including data cleaning and data preparation, easier. Let’s start by installing the tidyverse package using the install_packages() function in the same way as we installed the ggplot2 package last week. Install tidyverse package using the install_packages() function now. Note The tidyverse package is in fact a collection of packages that are specifically designed for data science tasks. Where in many cases different packages work all slightly differently, all packages of the tidyverse share the underlying design philosophy, grammar, and data structures. The ggplot2 package that you worked with last week is actually one of the core package of the tidyverse. This also means that if you load the tidyverse package through library(tidyverse) you directly have access to all the functions that are part of the ggplot2 package and you do not have to load the ggplot2 package separately. Because the tidyverse consists of multiple packages, it may take a little while before everything is installed so be patient! For more information on tidyverse, have a look at https://www.tidyverse.org/. If your installation was successful, you can now load the tidyverse as follows: # load the tidyverse library(tidyverse) ## ── Attaching core tidyverse packages ────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ lubridate 1.9.2 ✔ tibble 3.2.1 ## ✔ purrr 1.0.1 ✔ tidyr 1.3.0 ## ── Conflicts ──────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ lubridate::hms() masks vembedr::hms() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors Figure 4.3: Loading the tidyverse. Note After loading the tidyverse, you may get a short information messages to inform you which packages are attached to your R session and whether there are any conflicting functions. This simply means that there are functions that are named the same across the packages that you have loaded. For instance, when you load the tidyverse in a fresh R session, the tidyverse will tell you that two functions from the dplyr package (which is an integral part of the tidyverse package) mask the functionality of two functions of the stats package (which is part of base R). This simply means that if you now type in filter() you will get the dplyr functionality and not the functionality of the stats. This is very important information as both these functions do something completely different! You can still use the filter() function from the stats package but then you have to explicitly tell R that you want to use the function called filter() within the stats package: stats::filter(). Now that we have loaded the tidyverse we can continue with our data preparation. We will start by reading in the csv file that we created earlier, however, we will use the function read_csv() from the tidyverse instead of the function read.csv() from base R. The function read_csv returns a so-called tibble. Tibbles are data frames, but they tweak some older behaviours to make life a little easier. # load csv file from working directory midyear &lt;- read_csv(&#39;midyear2020_v2.csv&#39;) # inspect midyear ## # A tibble: 7,201 × 7 ## msoa_code msoa_name lad18_code lad18_name lad20_code lad20_name pop20 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 E02002483 Hartlepool 001 E06000001 Hartlepool E06000001 Hartlepool 10332 ## 2 E02002484 Hartlepool 002 E06000001 Hartlepool E06000001 Hartlepool 10440 ## 3 E02002485 Hartlepool 003 E06000001 Hartlepool E06000001 Hartlepool 8165 ## 4 E02002487 Hartlepool 005 E06000001 Hartlepool E06000001 Hartlepool 5174 ## 5 E02002488 Hartlepool 006 E06000001 Hartlepool E06000001 Hartlepool 5894 ## 6 E02002489 Hartlepool 007 E06000001 Hartlepool E06000001 Hartlepool 7638 ## 7 E02002490 Hartlepool 008 E06000001 Hartlepool E06000001 Hartlepool 6182 ## 8 E02002491 Hartlepool 009 E06000001 Hartlepool E06000001 Hartlepool 6450 ## 9 E02002492 Hartlepool 010 E06000001 Hartlepool E06000001 Hartlepool 6895 ## 10 E02002493 Hartlepool 011 E06000001 Hartlepool E06000001 Hartlepool 6722 ## # ℹ 7,191 more rows Questions Do you notice a difference between how base R prints a data frame and how a tibble gets printed? Let’s have a better look at our data particularly the data types. We can can do this through the str() function: # inspect str(midyear) ## spc_tbl_ [7,201 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ msoa_code : chr [1:7201] &quot;E02002483&quot; &quot;E02002484&quot; &quot;E02002485&quot; &quot;E02002487&quot; ... ## $ msoa_name : chr [1:7201] &quot;Hartlepool 001&quot; &quot;Hartlepool 002&quot; &quot;Hartlepool 003&quot; &quot;Hartlepool 005&quot; ... ## $ lad18_code: chr [1:7201] &quot;E06000001&quot; &quot;E06000001&quot; &quot;E06000001&quot; &quot;E06000001&quot; ... ## $ lad18_name: chr [1:7201] &quot;Hartlepool&quot; &quot;Hartlepool&quot; &quot;Hartlepool&quot; &quot;Hartlepool&quot; ... ## $ lad20_code: chr [1:7201] &quot;E06000001&quot; &quot;E06000001&quot; &quot;E06000001&quot; &quot;E06000001&quot; ... ## $ lad20_name: chr [1:7201] &quot;Hartlepool&quot; &quot;Hartlepool&quot; &quot;Hartlepool&quot; &quot;Hartlepool&quot; ... ## $ pop20 : num [1:7201] 10332 10440 8165 5174 5894 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. msoa_code = col_character(), ## .. msoa_name = col_character(), ## .. lad18_code = col_character(), ## .. lad18_name = col_character(), ## .. lad20_code = col_character(), ## .. lad20_name = col_character(), ## .. pop20 = col_number() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; The output of the str() function tells us that every column in our data frame (tibble) is of type “character”, which comes down to a variable that contains textual data. What is interesting, obviously, is that also our pop20 variable is considered a character variable. The reason for this is that in the original data white space is used as thousands separator (i.e. 11 000 instead of 11000 or 11,000). Does it matter? Well, let’s try to calculate the mean and median of our pop20 variable: # mean mean(midyear$pop20) ## [1] 8293.254 # median median(midyear$pop20) ## [1] 7974 Note Depending on computer settings and the version of Microsoft Excel or spreadsheet software that you use, in some case you could get a numeric variable (e.g. double) instead of a character variable for your pop120 variable. If so: you are lucky and do not need to undertake the steps below to turn this character column into a numeric column. (This will also mean that at this stage your results will differ from the results shown here.) Just continue with the tutorial by carefully reading through the steps that you would have needed to take if indeed your results would have been the same as shown here. As you can see, this is not working perfectly for the simple fact that the the mean() function requires a numeric variable as its input! The median() does return a value, however, this is not per se the median that we are interested in as the sorting of the data is now done alphabetically: see for your self what happens when using the sort() function on the pop20 variable. Fortunately, there are some functions in the dplyr package, which is part of the tidyverse, that will help us further cleaning and preparing our data. Some of the most important and useful functions are: Package Function Use to dplyr select select columns dplyr filter select rows dplyr mutate transform or recode variables dplyr summarise summarise data dplyr group by group data into subgroups for further processing Note Remember that when you encounter a function in a piece of R code that you have not seen before and you are wondering what it does that you can get access the documentation through ?name_of_function, e.g. ?mutate. For almost any R package, the documentation contains a list of arguments that the function takes, in which format the functions expects these arguments, as well as a set of usage examples. Let’s have a look at two of the dplyr functions: select() and filter(). # select columns with the select function # note that within dplyr we not use quotation marks to refer to columns midyear_sel &lt;- select(midyear,msoa_code,msoa_name,lad20_code,lad20_name,pop20) # inspect midyear_sel ## # A tibble: 7,201 × 5 ## msoa_code msoa_name lad20_code lad20_name pop20 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 E02002483 Hartlepool 001 E06000001 Hartlepool 10332 ## 2 E02002484 Hartlepool 002 E06000001 Hartlepool 10440 ## 3 E02002485 Hartlepool 003 E06000001 Hartlepool 8165 ## 4 E02002487 Hartlepool 005 E06000001 Hartlepool 5174 ## 5 E02002488 Hartlepool 006 E06000001 Hartlepool 5894 ## 6 E02002489 Hartlepool 007 E06000001 Hartlepool 7638 ## 7 E02002490 Hartlepool 008 E06000001 Hartlepool 6182 ## 8 E02002491 Hartlepool 009 E06000001 Hartlepool 6450 ## 9 E02002492 Hartlepool 010 E06000001 Hartlepool 6895 ## 10 E02002493 Hartlepool 011 E06000001 Hartlepool 6722 ## # ℹ 7,191 more rows # filter rows with the filter function # note that within dplyr we not use quotation marks to refer to columns midyear_fil &lt;- filter(midyear,lad20_name==&#39;Leeds&#39;) # inspect midyear_fil ## # A tibble: 107 × 7 ## msoa_code msoa_name lad18_code lad18_name lad20_code lad20_name pop20 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 E02002330 Leeds 001 E08000035 Leeds E08000035 Leeds 6663 ## 2 E02002331 Leeds 002 E08000035 Leeds E08000035 Leeds 7082 ## 3 E02002332 Leeds 003 E08000035 Leeds E08000035 Leeds 6202 ## 4 E02002333 Leeds 004 E08000035 Leeds E08000035 Leeds 7553 ## 5 E02002334 Leeds 005 E08000035 Leeds E08000035 Leeds 7160 ## 6 E02002335 Leeds 006 E08000035 Leeds E08000035 Leeds 7460 ## 7 E02002336 Leeds 007 E08000035 Leeds E08000035 Leeds 6585 ## 8 E02002337 Leeds 008 E08000035 Leeds E08000035 Leeds 7392 ## 9 E02002338 Leeds 009 E08000035 Leeds E08000035 Leeds 6769 ## 10 E02002339 Leeds 010 E08000035 Leeds E08000035 Leeds 5297 ## # ℹ 97 more rows Questions How can we get the same data frame with selected columns (midyear_sel) using the base R syntax we have been using in previous weeks? How can we get the same data frame with filtered rows (midyear_fil) using the base R syntax we have been using in previous weeks? With the mutate() function we can easily create new columns, so let’s try this function to see if we can create a new variable that contains all the data from pop20 in a numeric format. # create a new variable named pop19_no space # by replacing all white space with nothing using the &quot;str_replace_all&quot; function midyear &lt;- mutate(midyear, pop20_nospace=str_replace_all(pop20, pattern=&#39; &#39;, repl=&#39;&#39;)) # inspect midyear ## # A tibble: 7,201 × 8 ## msoa_code msoa_name lad18_code lad18_name lad20_code lad20_name pop20 pop20_nospace ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 E02002483 Hartlepool 001 E06000001 Hartlepool E06000001 Hartlepool 10332 10332 ## 2 E02002484 Hartlepool 002 E06000001 Hartlepool E06000001 Hartlepool 10440 10440 ## 3 E02002485 Hartlepool 003 E06000001 Hartlepool E06000001 Hartlepool 8165 8165 ## 4 E02002487 Hartlepool 005 E06000001 Hartlepool E06000001 Hartlepool 5174 5174 ## 5 E02002488 Hartlepool 006 E06000001 Hartlepool E06000001 Hartlepool 5894 5894 ## 6 E02002489 Hartlepool 007 E06000001 Hartlepool E06000001 Hartlepool 7638 7638 ## 7 E02002490 Hartlepool 008 E06000001 Hartlepool E06000001 Hartlepool 6182 6182 ## 8 E02002491 Hartlepool 009 E06000001 Hartlepool E06000001 Hartlepool 6450 6450 ## 9 E02002492 Hartlepool 010 E06000001 Hartlepool E06000001 Hartlepool 6895 6895 ## 10 E02002493 Hartlepool 011 E06000001 Hartlepool E06000001 Hartlepool 6722 6722 ## # ℹ 7,191 more rows # create a new variable named pop20_numeric # by transforming the pop20_nospace variable using the &quot;as.numeric&quot; function midyear &lt;- mutate(midyear,pop20_numeric=as.numeric(pop20_nospace)) # inspect midyear ## # A tibble: 7,201 × 9 ## msoa_code msoa_name lad18_code lad18_name lad20_code lad20_name pop20 pop20_nospace pop20_numeric ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 E02002483 Hartlepool 001 E06000001 Hartlepool E06000001 Hartlepool 10332 10332 10332 ## 2 E02002484 Hartlepool 002 E06000001 Hartlepool E06000001 Hartlepool 10440 10440 10440 ## 3 E02002485 Hartlepool 003 E06000001 Hartlepool E06000001 Hartlepool 8165 8165 8165 ## 4 E02002487 Hartlepool 005 E06000001 Hartlepool E06000001 Hartlepool 5174 5174 5174 ## 5 E02002488 Hartlepool 006 E06000001 Hartlepool E06000001 Hartlepool 5894 5894 5894 ## 6 E02002489 Hartlepool 007 E06000001 Hartlepool E06000001 Hartlepool 7638 7638 7638 ## 7 E02002490 Hartlepool 008 E06000001 Hartlepool E06000001 Hartlepool 6182 6182 6182 ## 8 E02002491 Hartlepool 009 E06000001 Hartlepool E06000001 Hartlepool 6450 6450 6450 ## 9 E02002492 Hartlepool 010 E06000001 Hartlepool E06000001 Hartlepool 6895 6895 6895 ## 10 E02002493 Hartlepool 011 E06000001 Hartlepool E06000001 Hartlepool 6722 6722 6722 ## # ℹ 7,191 more rows # inspect str(midyear) ## tibble [7,201 × 9] (S3: tbl_df/tbl/data.frame) ## $ msoa_code : chr [1:7201] &quot;E02002483&quot; &quot;E02002484&quot; &quot;E02002485&quot; &quot;E02002487&quot; ... ## $ msoa_name : chr [1:7201] &quot;Hartlepool 001&quot; &quot;Hartlepool 002&quot; &quot;Hartlepool 003&quot; &quot;Hartlepool 005&quot; ... ## $ lad18_code : chr [1:7201] &quot;E06000001&quot; &quot;E06000001&quot; &quot;E06000001&quot; &quot;E06000001&quot; ... ## $ lad18_name : chr [1:7201] &quot;Hartlepool&quot; &quot;Hartlepool&quot; &quot;Hartlepool&quot; &quot;Hartlepool&quot; ... ## $ lad20_code : chr [1:7201] &quot;E06000001&quot; &quot;E06000001&quot; &quot;E06000001&quot; &quot;E06000001&quot; ... ## $ lad20_name : chr [1:7201] &quot;Hartlepool&quot; &quot;Hartlepool&quot; &quot;Hartlepool&quot; &quot;Hartlepool&quot; ... ## $ pop20 : num [1:7201] 10332 10440 8165 5174 5894 ... ## $ pop20_nospace: chr [1:7201] &quot;10332&quot; &quot;10440&quot; &quot;8165&quot; &quot;5174&quot; ... ## $ pop20_numeric: num [1:7201] 10332 10440 8165 5174 5894 ... Note the information for pop20_numeric: str indicates that the column is now of type num (i.e. numeric). This is much better. We can now try to calculate the median and the mean again: # calculate the mean of the pop19_numeric variable mean(midyear$pop20_numeric) ## [1] 8293.254 # calculate the median of the pop19_numeric variable median(midyear$pop20_numeric) ## [1] 7974 Everything seems to be working fine now and our data is now finally ready for analysis, so it is probably wise to save’s save it. # write to new csv write.csv(midyear, &#39;midyear2020_clean.csv&#39;, row.names=FALSE) Recap In this section you have learnt how to: Transform your data frame to a tibble. Use some of the functionality of the tidyverse to prepare your data set and make it suitable for analysis. 4.4 Seminar Please find the seminar task and seminar questions for this week’s seminar below. Note Please make sure that you have executed the seminar task and have answered the seminar questions before the seminar! Seminar task Of course, we did not go through all the trouble of downloading and preparing this file without using it. Use the cleaned version of the midyear data set that we just created to: Create a new object / data set that only contains data for the Local Authority District (2020 boundaries) of Manchester. Create a new object / data set that only contains data for the Local Authority District (2020 boundaries) of Birmingham. For both new objects: Calculate the mean, median, and standard deviation of the pop20_numeric variable. Create a boxplot of the pop20_numeric variable using the ggplot2 package. Create a histogram of the pop20_numeric variable using the ggplot2 package. Select a bin width that you think is appropriate. Seminar questions Compare the results of the descriptive statistics you have calculated for your Birmingham object / data set with the results of the descriptive statistics you have calculated for you Manchester data set. What do these descriptive statistics tell you? Why did you select the bin width that you used for creating your histograms? Compare the histograms that you created for your Birmingham object / data set and the Manchester data set, what can you tell about the population distribution of both Local Authority Districts? Additional tasks (in your own time) If you found these assignments and questions relatively easy and want an assignment that is a little more realistic, try to do the following: Download the mid-year population estimates for 2018 to your computer (Mid-2018: SAPE21DT4 (unformatted)). Make sure that you download the file with exactly this name: it is listed under the “supporting files you may find useful” heading. Prepare a csv file of the first three columns of the Mid-2018 Perons tab and import into R. Give the columns the following names: msoa_code, msoa_name, pop18. Take the necessary steps in R to make sure that all numeric variables are also recognised by R as numeric variables. Join the 2018 and 2019 midyear population data set by using the left join() function from the tidyverse() package, e.g. something like: midyear &lt;- left_join(midyear2018, midyear2019, by=c('msoa_code'='msoa_code'). Calculate the population change between 2018 and 2019. Create a histogram of the population change. Which MSOA has the largest decline in population between 2018 and 2019? Which MSOA had the largest increase in population between 2018 and 2019? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
